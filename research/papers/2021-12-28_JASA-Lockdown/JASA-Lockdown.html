<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andrew Mitchell">
<meta name="author" content="Tin Oberman">
<meta name="author" content="Francesco Aletta">
<meta name="author" content="Magdalena Kachlicka">
<meta name="author" content="Matteo Lionello">
<meta name="author" content="Mercede Erfanian">
<meta name="author" content="Jian Kang">
<meta name="dcterms.date" content="2021-12-28">
<meta name="keywords" content="Soundscape, Psychological acoustics, Acoustic modeling, Acoustic noise, Acoustic ecology, Signal processing, Urban development, Regression analysis">
<meta name="description" content="Published in the Journal of the Acoustical Society of America in December, 2021. Part of a special issue on COVID-19 Pandemic Effects. ">

<title>Investigating urban soundscapes of the COVID-19 lockdown: A predictive soundscape modeling approach – Andrew Mitchell</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Investigating urban soundscapes of the COVID-19 lockdown: A predictive soundscape modeling approach – Andrew Mitchell">
<meta property="og:description" content="Published in the Journal of the Acoustical Society of America in December, 2021. Part of a special issue on COVID-19 Pandemic Effects. ">
<meta property="og:image" content="https://drandrewmitchell.com/research/papers/2021-12-28_JASA-Lockdown/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta property="og:site_name" content="Andrew Mitchell">
<meta name="twitter:title" content="Investigating urban soundscapes of the COVID-19 lockdown: A predictive soundscape modeling approach – Andrew Mitchell">
<meta name="twitter:description" content="Published in the Journal of the Acoustical Society of America in December, 2021. Part of a special issue on COVID-19 Pandemic Effects. ">
<meta name="twitter:image" content="https://drandrewmitchell.com/research/papers/2021-12-28_JASA-Lockdown/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta name="twitter:creator" content="@acousticsman">
<meta name="twitter:site" content="@acousticsman">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Andrew Mitchell</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-research" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Research</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-research">    
        <li>
    <a class="dropdown-item" href="../../../research/papers.html">
 <span class="dropdown-text">Open Source Papers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../research/presentations.html">
 <span class="dropdown-text">Talks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../research/list-of-pubs.html">
 <span class="dropdown-text">List of Publications</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../software.html"> 
<span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../resources.html"> 
<span class="menu-text">Resources</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/MitchellAcoustics/quarto-website">
            Source Code
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/MitchellAcoustics/quarto-website/issues">
            Report a bug
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title"><p>Investigating urban soundscapes of the COVID-19 lockdown: A predictive soundscape modeling approach</p></h1>
                  <div>
        <div class="description">
          Published in the Journal of the Acoustical Society of America in December, 2021. Part of a special issue on COVID-19 Pandemic Effects. <span class="__dimensions_badge_embed__" data-doi="10.1121/10.0009794" data-hide-zero-citations="true" data-style="small_rectangle"></span>
          <script async="" src="https://badge.dimensions.ai/badge.js" charset="utf-8"></script>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">journal-articles</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Andrew Mitchell <a href="mailto:andrew.mitchell.18@ucl.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-0978-5046" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University College London
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Tin Oberman <a href="mailto:t.oberman@ucl.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University College London
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Francesco Aletta <a href="mailto:f.aletta@ucl.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-0351-3189" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University College London
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Magdalena Kachlicka </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University College London
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Matteo Lionello </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University College London
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Mercede Erfanian </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University College London
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Jian Kang <a href="mailto:j.kang@ucl.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0001-8995-5636" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University College London
            </p>
        </div>
    </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 28, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">Abstract</div>
      <p>The unprecedented lockdowns due to COVID-19 in spring 2020 triggered changes in human activities in public spaces. A predictive modeling approach was developed to characterize the changes in the perception of the sound environment when people could not be surveyed. Building on a database of soundscape questionnaires (<span class="math inline">N=1,136</span>) and binaural recordings (<span class="math inline">N=687</span>) collected in 13 locations across London and Venice during 2019, new recordings (<span class="math inline">N=571</span>) were made in the same locations during the 2020 lockdowns. Using these 30-second-long recordings, linear multi-level models were developed to predict soundscape pleasantness (<span class="math inline">R^2=0.85</span>) and eventfulness (<span class="math inline">R^2=0.715</span>) during the lockdown and compare changes for each location. Performance was above average for comparable models. An online listening study also investigated the change in sound sources within the spaces. Results indicate: 1) human sounds were less dominant and natural sounds more dominant across all locations; 2) contextual information is important for predicting pleasantness but not for eventfulness; 3) perception shifted towards less eventful soundscapes and to more pleasant soundscapes for previously traffic-dominated locations, but not for human- and natural-dominated locations. This study demonstrates the usefulness of predictive modeling and the importance of considering contextual information when discussing the impact of sound level reductions on the soundscape.</p>
    </div>
  </div>

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Soundscape, Psychological acoustics, Acoustic modeling, Acoustic noise, Acoustic ecology, Signal processing, Urban development, Regression analysis</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#sec-intro" id="toc-sec-intro" class="nav-link active" data-scroll-target="#sec-intro"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#materials-and-methods" id="toc-materials-and-methods" class="nav-link" data-scroll-target="#materials-and-methods"><span class="header-section-number">2</span> Materials and methods</a>
  <ul class="collapse">
  <li><a href="#onsite-data-questionnaires-binaural-measurements-and-recordings" id="toc-onsite-data-questionnaires-binaural-measurements-and-recordings" class="nav-link" data-scroll-target="#onsite-data-questionnaires-binaural-measurements-and-recordings"><span class="header-section-number">2.1</span> Onsite data: Questionnaires, binaural measurements, and recordings</a></li>
  <li><a href="#modelling" id="toc-modelling" class="nav-link" data-scroll-target="#modelling"><span class="header-section-number">2.2</span> Modelling</a></li>
  <li><a href="#online-survey" id="toc-online-survey" class="nav-link" data-scroll-target="#online-survey"><span class="header-section-number">2.3</span> Online survey</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">3</span> Results</a>
  <ul class="collapse">
  <li><a href="#perceived-sound-source-dominance" id="toc-perceived-sound-source-dominance" class="nav-link" data-scroll-target="#perceived-sound-source-dominance"><span class="header-section-number">3.1</span> Perceived sound source dominance</a></li>
  <li><a href="#model-selection-performance-and-application" id="toc-model-selection-performance-and-application" class="nav-link" data-scroll-target="#model-selection-performance-and-application"><span class="header-section-number">3.2</span> Model selection, performance, and application</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">4</span> Discussion</a>
  <ul class="collapse">
  <li><a href="#interpretation-of-the-results" id="toc-interpretation-of-the-results" class="nav-link" data-scroll-target="#interpretation-of-the-results"><span class="header-section-number">4.1</span> Interpretation of the results</a></li>
  <li><a href="#limitations-of-the-study" id="toc-limitations-of-the-study" class="nav-link" data-scroll-target="#limitations-of-the-study"><span class="header-section-number">4.2</span> Limitations of the study</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">5</span> Conclusion</a></li>
  
  
  
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="JASA-Lockdown.pdf"><i class="bi bi-file-pdf"></i>PDF (elsevier)</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<section id="sec-intro" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="sec-intro"><span class="header-section-number">1</span> Introduction</h2>
<p>The global emergency caused by the COVID-19 pandemic in early 2020 required national lockdown measures across the world, primarily targeting human activity. In the United Kingdom, construction and transport were allowed to continue, but a decrease in activity was observed <span class="citation" data-cites="Hadjidemetriou2020impact">(<a href="#ref-Hadjidemetriou2020impact" role="doc-biblioref">Hadjidemetriou et al. 2020</a>)</span>. In other countries, such as Italy, the restrictions were more severe and even included limiting people’s movement to a certain radius from their place of residence <span class="citation" data-cites="Ren2020Pandemic">(<a href="#ref-Ren2020Pandemic" role="doc-biblioref">Ren 2020</a>)</span>. The explorations in environmental acoustics of lockdown conditions across the world have revealed various degrees of impact on the acoustic environment, with researchers reporting reductions in noise levels affecting the population at the scale of urban agglomerations such as Ruhr Area in Germany <span class="citation" data-cites="Hornberg2021Impact">(<a href="#ref-Hornberg2021Impact" role="doc-biblioref">Hornberg et al. 2021</a>)</span> and conurbations in the south of France <span class="citation" data-cites="Munoz2020Lockdown">(<a href="#ref-Munoz2020Lockdown" role="doc-biblioref">Munoz et al. 2020</a>)</span>. Impacts have also been reported at a scale of a multimillion city such as Madrid <span class="citation" data-cites="Asensio2020Changes">(<a href="#ref-Asensio2020Changes" role="doc-biblioref">Asensio, Pavón, and Arcas 2020</a>)</span> or Barcelona <span class="citation" data-cites="BonetSola2021Soundscape">(<a href="#ref-BonetSola2021Soundscape" role="doc-biblioref">Bonet-Solà et al. 2021</a>)</span> as well as at a more local, city-center or even public space-scale in cities such as Stockholm <span class="citation" data-cites="Rumpler2021Noise">(<a href="#ref-Rumpler2021Noise" role="doc-biblioref">Rumpler, Venkataraman, and Göransson 2021</a>)</span>, London <span class="citation" data-cites="Aletta2020Assessing">(<a href="#ref-Aletta2020Assessing" role="doc-biblioref">Aletta et al. 2020</a>)</span>, Girona <span class="citation" data-cites="AlsinaPages2021Changes">(<a href="#ref-AlsinaPages2021Changes" role="doc-biblioref">Alsina-Pagès, Bergadà, and Martı́nez-Suquı́a 2021</a>)</span>, or Granada <span class="citation" data-cites="VidaManzano2021sound">(<a href="#ref-VidaManzano2021sound" role="doc-biblioref">Vida Manzano et al. 2021</a>)</span>. In general, these studies have demonstrated a decrease in urban noise levels and indicated a difference in the amount the level decreased depending on the type of space investigated (e.g.&nbsp;parks, urban squares, etc.) and the type of human activity characteristic for the space, with higher reductions in places typically associated with human sounds and activities such as shopping and tourism.</p>
<p>Those studies were mostly focused around the <span class="math inline">L_{Aeq}</span>, as well as a standardization approach to reporting subsequent changes in soundscape proposed by <span class="citation" data-cites="Asensio2020Taxonomy">Asensio et al. (<a href="#ref-Asensio2020Taxonomy" role="doc-biblioref">2020</a>)</span>. They were not able to reveal the perceptual impact of such conditions in public spaces also because of: 1) the lack of subjective data for the exact or comparable locations in previous years; and 2) the lack of participants present in public spaces during the lockdown, hence the inability to collect soundscape data in situ. <span class="citation" data-cites="Munoz2020Lockdown">Munoz et al. (<a href="#ref-Munoz2020Lockdown" role="doc-biblioref">2020</a>)</span> combined noise measurements with an online questionnaire deployed to residents, some of which were residing in the areas covered by the noise monitoring network available. The participants were asked to recall how their lockdown area sounded before and during the first lockdown in 2020 and to describe the perceived change. They observed a consistent reduction in levels, followed by the perceived reduction of transport sounds (air and road) and an increase of natural sounds, while the resulting environment was described as pleasant, calm, and peaceful. By combining field recordings and focus groups, <span class="citation" data-cites="Sakagami2020How">Sakagami (<a href="#ref-Sakagami2020How" role="doc-biblioref">2020</a>)</span> and <span class="citation" data-cites="Lenzi2021Soundscape">Lenzi, Sádaba, and Lindborg (<a href="#ref-Lenzi2021Soundscape" role="doc-biblioref">2021</a>)</span> observed changes in the sound source composition and the affective quality of soundscape in a residential area in Kobe, Japan and a public space in Getxa, Spain, respectively, during the different stages of the lockdown period. Following the easing of lockdown measures, a decrease in animal and traffic sounds was observed in Kobe, while an increase in eventfulness, loudness, and presence of human sound sources, followed by a decrease in pleasantness, was shown in Getxa.</p>
<p><span class="citation" data-cites="Aletta2020Assessing">Aletta et al. (<a href="#ref-Aletta2020Assessing" role="doc-biblioref">2020</a>)</span> explored the impacts of the COVID-19 lockdowns on the acoustic environment in London in particular, through many short-term (30s) binaural recordings. This study revealed that average reductions in the various locations considered ranged from 10.7 dB (<span class="math inline">L_{Aeq}</span>) to 1.2 dB, with an overall average reduction of 5.4 dB. This metric-reporting focused approach left the following research questions unanswered: how would people have perceived these spaces as a result of this change in acoustic environment (RQ1), and would these sound level reductions result in improvements to the soundscape of the spaces (RQ2)? The 1st research question (RQ1), addressing the perceptual effect of the change in urban soundscape induced by the lockdowns, can be further broken down into the following questions: how was the sound source composition influenced by the change; how would the affective response to the acoustic environment in lockdowns change; and could this demonstrate the effect of human activities on the perception of an acoustic environment in general?</p>
<p>These questions arise out of the soundscape approach, which is characterized by prioritizing the perceptual effect of an acoustic environment by taking into account the interaction of sound sources, context, and the person perceiving it <span class="citation" data-cites="ISO12913Part1 Truax1999Handbook">(<a href="#ref-ISO12913Part1" role="doc-biblioref">ISO 12913-1:2014 2014</a>; <a href="#ref-Truax1999Handbook" role="doc-biblioref">Truax 1999</a>)</span>, bringing together objective and subjective factors. The soundscape approach to noise mitigation and management is being recognized as a response to arising environmental requirements on noise pollution and sustainability, such as the regulation of quiet areas in Europe <span class="citation" data-cites="EuropeanUnion2002Directive Kang2018Impact Radicchi2021Sound">(<a href="#ref-EuropeanUnion2002Directive" role="doc-biblioref">European Union 2002</a>; <a href="#ref-Kang2018Impact" role="doc-biblioref">Kang and Aletta 2018</a>; <a href="#ref-Radicchi2021Sound" role="doc-biblioref">Radicchi et al. 2021</a>)</span>. This has been further formalized in <span class="citation" data-cites="ISO12913Part2">ISO/TS 12913-2:2018 (<a href="#ref-ISO12913Part2" role="doc-biblioref">2018</a>)</span> via the adoption of the circumplex model of soundscape <span class="citation" data-cites="Axelsson2010principal">(<a href="#ref-Axelsson2010principal" role="doc-biblioref">Axelsson, Nilsson, and Berglund 2010</a>)</span>, in which the perception of a soundscape can be described in terms of its pleasantness and eventfulness, as one of the standard methods of soundscape assessment.</p>
<p>Soundscape research is therefore traditionally rooted in environmental acoustics and environmental psychology, typically dealing with outdoor spaces <span class="citation" data-cites="Torresin2020Indoor">(<a href="#ref-Torresin2020Indoor" role="doc-biblioref">Torresin et al. 2020</a>)</span> and urban open spaces, where parks and squares are often used as case study sites <span class="citation" data-cites="Kang2006Urban">(<a href="#ref-Kang2006Urban" role="doc-biblioref">Kang 2006</a>)</span>. A soundscape assessment typically requires people to be surveyed but the presence of people at a location influences assessment <span class="citation" data-cites="Aletta2018Towards">(<a href="#ref-Aletta2018Towards" role="doc-biblioref">Aletta and Kang 2018</a>)</span> and ‘quiet places’ usually require low numbers of users to remain quiet, which limits the possibility of an assessment. Even in a crowded public space, soundscape surveys are demanding as they require significant resources to carry out at scale, limiting their widespread application <span class="citation" data-cites="Mitchell2020Soundscape">(<a href="#ref-Mitchell2020Soundscape" role="doc-biblioref">Mitchell et al. 2020</a>)</span>. Therefore, a need for a predictive model arises to overcome this limitation and improve the implementation of the soundscape approach into everyday planning and management practices.</p>
<p>According to a recent review of predictive soundscape models from <span class="citation" data-cites="Lionello2020systematic">Lionello, Aletta, and Kang (<a href="#ref-Lionello2020systematic" role="doc-biblioref">2020</a>)</span>, the degree of employing auditory and non-auditory factors in soundscape prediction varies with some studies relying on contextual, personal/demographic <span class="citation" data-cites="Erfanian2021Psychological Tarlao2020Investigating">(<a href="#ref-Erfanian2021Psychological" role="doc-biblioref">Erfanian et al. 2021</a>; <a href="#ref-Tarlao2020Investigating" role="doc-biblioref">Tarlao, Steffens, and Guastavino 2020</a>)</span> or social media <span class="citation" data-cites="Aiello2016Chatty">(<a href="#ref-Aiello2016Chatty" role="doc-biblioref">Aiello et al. 2016</a>)</span> data entirely to predict and generate soundscape features. Some methods also incorporate perceptually-derived features, such as subjective sound level and visual pleasantness as predictors <span class="citation" data-cites="Lionello2020systematic">(<a href="#ref-Lionello2020systematic" role="doc-biblioref">Lionello, Aletta, and Kang 2020</a>)</span>. In general, these methods which incorporate perceptually-derived inputs achieve better accuracy rates than those which don’t, however this perception information must also be obtained from people via a survey and therefore are unsuitable for predictive modeling where surveys are not possible. For example, <span class="citation" data-cites="Ricciardi2015Sound">Ricciardi et al. (<a href="#ref-Ricciardi2015Sound" role="doc-biblioref">2015</a>)</span> proposed two models based on data collected from a smartphone application to predict urban sound quality indicators based on linear regressions. The first model which incorporated perceptually-derived input features (visual quality and familiarity) achieved an <span class="math inline">R^2</span> of 0.72, while a second model without these features achieved an <span class="math inline">R^2</span> of 0.58. This indicates the necessity for considering and accounting for the influence which contextual factors in a space have on the relationship between the sound environment itself and the listener’s perception of it (i.e.&nbsp;the soundscape) while also highlighting the challenges associated with a predictive model which depends only on measurable features.</p>
<p>Therefore, a third research question arises: what are the key features needed for a soundscape prediction model based on comprehensive acoustic on-site measurements to be used for assessing locations with low social presence or in situations where conducting surveys is impractical (RQ3)?</p>
</section>
<section id="materials-and-methods" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="materials-and-methods"><span class="header-section-number">2</span> Materials and methods</h2>
<p>This study was conducted via initial onsite data collection campaigns in Central London and Venice in 2019 before the outbreak of COVID-19 as part of the Soundscape Indices (SSID) project <span class="citation" data-cites="Mitchell2020Soundscape">(<a href="#ref-Mitchell2020Soundscape" role="doc-biblioref">Mitchell et al. 2020</a>)</span> and in 2020 during the strictest part of the lockdowns <span class="citation" data-cites="Aletta2020Assessing">(<a href="#ref-Aletta2020Assessing" role="doc-biblioref">Aletta et al. 2020</a>)</span>, including objective acoustic data (2019 and 2020) and subjective responses (2019 only). The full in situ dataset, as described in this section, has been made publicly available as ‘The International Soundscape Database (V0.2.1)’ on Zenodo<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a><span class="citation" data-cites="Mitchell2021International">(<a href="#ref-Mitchell2021International" role="doc-biblioref">Mitchell et al. 2021</a>)</span>.</p>
<p>Using both 2019 and 2020 binaural recordings, an online listening experiment was conducted to provide an understanding about the change in sound source composition. The 2019 onsite questionnaire data were used to define the dominant sound source at each location as a starting point for interpreting soundscape change. A predictive model was developed to reveal the change in the perceived pleasantness and eventfulness using objective acoustic data and location to predict subjective responses. Although the initial (2019) dataset contains additional locations (specifically, in Spain, the Netherlands, and China), due to the nature of this study as a reaction to the strict movement and activity restrictions, the sites which could be included in the lockdown (2020) measurement campaigns were limited to locations where staff and equipment had access and where recordings could be undertaken during the spring of 2020.</p>
<p>The sites were selected to provide a mixture of sizes and uses, varying in typology ranging from paved squares to small and large parks to waterside spaces across both cities. Throughout the text they are indexed via a LocationID based on the location’s name (e.g.&nbsp;CamdenTown, SanMarco), while a more in-depth overview of each is given in supplementary material<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. London is taken as an example of a large, typically noisy city while the Venice sample provides a unique look at spaces with typically very high human activity levels and no road traffic activity. In particular, the 2019 Venice surveys were taken to coincide with the yearly Carnevale festival in order to capture its distinct soundscape.</p>
<p>The ISO/TS 12913 <span class="citation" data-cites="ISO12913Part2">(<a href="#ref-ISO12913Part2" role="doc-biblioref">ISO/TS 12913-2:2018 2018</a>)</span> series were consulted for reporting on soundscape data. A detailed description of the 2019 survey campaigns is featured throughout the paper and in the public database. This study was approved by departmental UCL IEDE Ethics Committee on 17th July 2018 for onsite data collection and on the 2nd of June 2020 for the on-line listening experiment and is conducted in adherence to the ethical requirements of the Declaration of Helsinki <span class="citation" data-cites="WMA2013World">(<a href="#ref-WMA2013World" role="doc-biblioref">World Medical Association 2013</a>)</span>.</p>
<section id="onsite-data-questionnaires-binaural-measurements-and-recordings" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="onsite-data-questionnaires-binaural-measurements-and-recordings"><span class="header-section-number">2.1</span> Onsite data: Questionnaires, binaural measurements, and recordings</h3>
<p>The initial onsite data collection featured both questionnaire data collected from the general public and acoustic measurements, conducted across thirteen urban locations (in London <span class="math inline">N=11</span>, in Venice <span class="math inline">N=2</span>) between the 28th of February and the 21st of June 2019, with additional sessions in July and October 2019. Although the total survey period in 2019 extended over several seasons, the surveys at any individual location did not extend over seasons with different occupancy patterns. A total of 1,318 questionnaire responses were collected from the general population across the measurement points during 1 – 3 hour-long campaigns in both cities in 2019, accompanied by 693 approximately 30-second long 24-bit 44.1 kHz binaural recordings. After data cleaning, each of the 13 locations was characterized by between 14 to 80 recordings and between 24 to 147 questionnaire responses. Mean age of the participants was 33.8, with a standard deviation of 14.57 (45% male, 53.8% female, 0.4% non-conforming, 0.9% prefer-not-to-say).</p>
<p>Although recent results from both <span class="citation" data-cites="Tarlao2020Investigating">Tarlao, Steffens, and Guastavino (<a href="#ref-Tarlao2020Investigating" role="doc-biblioref">2020</a>)</span> and <span class="citation" data-cites="Erfanian2021Psychological">Erfanian et al. (<a href="#ref-Erfanian2021Psychological" role="doc-biblioref">2021</a>)</span> indicate the important influence of personal and demographic factors – in particular age and gender – on soundscape perception, these factors were not included as potential features in the modeling process. Given the nature of this study as addressing a scenario when people could not be surveyed, no additional demographic information is available in the lockdown case to be fed into the model and is therefore not useful to include for the development and application of this specific predictive model. This information is reported throughout the study simply to provide further context to the data collection.</p>
<p>The subsequent measurement campaign in 2020 mimicked the binaural recording strategy applied in the initial campaign and was performed between the 6th and the 25th of April 2020 in both cities, this time excluding the questionnaire. An additional 571 binaural recordings were collected on-site in 2020.</p>
<section id="data-collection" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="data-collection"><span class="header-section-number">2.1.1</span> Data collection</h4>
<p>The 2019 data collection was performed across all the locations using the protocol based on the Method A of the <span class="citation" data-cites="ISO12913Part2">ISO/TS 12913-2:2018 (<a href="#ref-ISO12913Part2" role="doc-biblioref">2018</a>)</span>, as described in <span class="citation" data-cites="Aletta2020Assessing">Aletta et al. (<a href="#ref-Aletta2020Assessing" role="doc-biblioref">2020</a>)</span> and <span class="citation" data-cites="Mitchell2020Soundscape">Mitchell et al. (<a href="#ref-Mitchell2020Soundscape" role="doc-biblioref">2020</a>)</span>, collected either via handheld tablets or paper copies of the questionnaire. The full questionnaire and data collection procedure are given in <span class="citation" data-cites="Mitchell2020Soundscape">Mitchell et al. (<a href="#ref-Mitchell2020Soundscape" role="doc-biblioref">2020</a>)</span>, however the key parts used for this study are those addressing sound source dominance and perceived affective quality (PAQ).</p>
<p>Participants are first asked to rate the perceived dominance of several sound sources, as assessed via a 5-point Likert scale, coded from 1 (Not at all) to 5 (Dominates completely). The sound sources are split into four categories: Traffic noise, Other noise, Human sounds, and Natural sounds and each is rated separately. Next are the 8 PAQs which make up the circumplex model of soundscape <span class="citation" data-cites="Axelsson2010principal">(<a href="#ref-Axelsson2010principal" role="doc-biblioref">Axelsson, Nilsson, and Berglund 2010</a>)</span>: pleasant, chaotic, vibrant, uneventful, calm, annoying, eventful, and monotonous. These are assessed on a 5-point Likert scale from 1 (Strongly disagree) to 5 (Strongly agree). In order to simplify the results and allow for modeling the responses as continuous values, the 8 PAQs undergo a trigonometric projection to reduce them onto the two primary dimensions of pleasant and eventful, according to the procedure outlined in Part 3 of the ISO 12913 series <span class="citation" data-cites="ISO12913Part3">(<a href="#ref-ISO12913Part3" role="doc-biblioref">ISO/TS 12913-3:2019 2019</a>)</span>. In order to distinguish the projected values from the Likert-scale PAQ responses, the projected values will be referred to as ISOPleasant and ISOEventful and can be considered to form an x-y coordinate point (x = ISOPleasant, y = ISOEventful) as explained in detail in <span class="citation" data-cites="Lionello2021Introducing">Lionello et al. (<a href="#ref-Lionello2021Introducing" role="doc-biblioref">2021</a>)</span>.</p>
<p>The calibrated binaural device SQobold with BHS II by Head Acoustics was used in both campaigns at all the locations by various operators to capture acoustic data, as mentioned in the acknowledgments. Following the established onsite protocol <span class="citation" data-cites="Mitchell2020Soundscape">(<a href="#ref-Mitchell2020Soundscape" role="doc-biblioref">Mitchell et al. 2020</a>)</span>, when participants were stopped in a group and filled in their responses simultaneously, a single binaural recording was used to capture their experience as a group. The purpose behind this sampling strategy was to obtain data from the perspective of a typical user, corresponding to a range of individual experiences available within an urban open space. These recordings are indexed by a GroupID such that the recording for each group is matched up to each of the corresponding respondents and their individual survey responses.</p>
</section>
<section id="data-cleaning" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="data-cleaning"><span class="header-section-number">2.1.2</span> Data cleaning</h4>
<p>The cleaning of the samples was conducted using the ArtemiS SUITE 11. The researcher discarded or cropped whole recordings, or its parts affected by wind gusts or containing noises and speech generated by the recording operator by accident or for the purpose of explaining the questionnaire to a participant. This resulted in 1,258 binaural recordings then processed further, as described in <a href="#sec-psychoacousticAnalysis" class="quarto-xref">Section&nbsp;2.1.3</a>. Psychoacoustic analyses are shown in the publicly available database<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>In order to maintain data quality and exclude cases where respondents either clearly did not understand the PAQ adjectives or intentionally misrepresented their answers, surveys for which the same response was given for every PAQ (e.g.&nbsp;‘Strongly agree’ to all 8 attributes) were excluded prior to calculating the ISO projected values. This is justified as no reasonable respondent who understood the questions would answer that they ‘strongly agree’ that a soundscape is pleasant and annoying, calm and chaotic, etc. Cases where respondents answered ‘Neutral’ to all PAQs are not excluded in this way, as a neutral response to all attributes is not necessarily contradictory. In addition, surveys were discarded as incomplete if more than 50% of the PAQ and sound source questions were not completed.</p>
<p>The site characterization per <span class="citation" data-cites="ISO12913Part2">ISO/TS 12913-2:2018 (<a href="#ref-ISO12913Part2" role="doc-biblioref">2018</a>)</span> is available in the supplementary material<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and public database<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, featuring the address, overall psychoacoustic characteristics of the location, typical use of each location, and pictures taken during the survey sessions.</p>
</section>
<section id="sec-psychoacousticAnalysis" class="level4" data-number="2.1.3">
<h4 data-number="2.1.3" class="anchored" data-anchor-id="sec-psychoacousticAnalysis"><span class="header-section-number">2.1.3</span> Psychoacoustic analyses</h4>
<p>The binaural recordings were analyzed in ArtemiS SUITE 11 to calculate the suite of 11 acoustic and psychoacoustic features given in <a href="#tbl-psychoacoustics" class="quarto-xref">Table&nbsp;1</a> to be used as initial predictors. The (psycho)acoustic predictors investigated were selected in order to describe many aspects of the recorded sound – in particular, the goal was to move beyond a focus on sound level, which currently dominates the existing literature on the acoustic effects of lockdowns noted in <a href="#sec-intro" class="quarto-xref">Section&nbsp;1</a>. In all, they are expected to reflect the sound level (<span class="math inline">L_{Aeq}</span>), perceived sound level (Loudness), spectral content (Sharpness, <span class="math inline">L_{Ceq}-L_{Aeq}</span>, Tonality), temporal character or predictability (Impulsiveness, Fluctuation Strength, Relative Approach), and overall annoyance (Psychoacoustic Annoyance). These metrics have been proposed as indicators to predict perceptual constructs of the soundscape <span class="citation" data-cites="Aletta2016Soundscape Aletta2017Dimensions">(<a href="#ref-Aletta2016Soundscape" role="doc-biblioref">Aletta, Kang, and Axelsson 2016</a>; <a href="#ref-Aletta2017Dimensions" role="doc-biblioref">Aletta, Axelsson, and Kang 2017</a>)</span> and have shown promise when combined together to form a more comprehensive model applied to real-world sounds <span class="citation" data-cites="Orga2021Multilevel">(<a href="#ref-Orga2021Multilevel" role="doc-biblioref">Orga et al. 2021</a>)</span>. The maximum value from the left and right channels of the binaural recording are used, as suggested in <span class="citation" data-cites="ISO12913Part3">ISO/TS 12913-3:2019 (<a href="#ref-ISO12913Part3" role="doc-biblioref">2019</a>)</span>.</p>
<div id="tbl-psychoacoustics" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-psychoacoustics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Psychoacoustic features considered for inclusion in the predictive models. All metrics are calculated for the full length of the recording (<span class="math inline">\sim30s</span>). As recommended by <span class="citation" data-cites="ISO2017Acoustics">ISO 532-1:2017 (<a href="#ref-ISO2017Acoustics" role="doc-biblioref">2017</a>)</span> and <span class="citation" data-cites="ISO12913Part2">ISO/TS 12913-2:2018 (<a href="#ref-ISO12913Part2" role="doc-biblioref">2018</a>)</span>, the 5th percentile of Loudness is used rather than the average.
</figcaption>
<div aria-describedby="tbl-psychoacoustics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 22%">
<col style="width: 11%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Feature</strong></th>
<th style="text-align: center;"><strong>Symbol</strong></th>
<th style="text-align: center;"><strong>Unit</strong></th>
<th><strong>Calculation Method</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Loudness (5th percentile)</td>
<td style="text-align: center;"><span class="math inline">N_5</span></td>
<td style="text-align: center;">sones</td>
<td><span class="citation" data-cites="ISO2017Acoustics">ISO 532-1:2017 (<a href="#ref-ISO2017Acoustics" role="doc-biblioref">2017</a>)</span></td>
</tr>
<tr class="even">
<td>Sharpness</td>
<td style="text-align: center;"><span class="math inline">S</span></td>
<td style="text-align: center;">acum</td>
<td><span class="citation" data-cites="ISO2017Acoustics">ISO 532-1:2017 (<a href="#ref-ISO2017Acoustics" role="doc-biblioref">2017</a>)</span></td>
</tr>
<tr class="odd">
<td>Roughness</td>
<td style="text-align: center;"><span class="math inline">R</span></td>
<td style="text-align: center;">asper</td>
<td><span class="citation" data-cites="Sottek2005Models">Sottek and Genuit (<a href="#ref-Sottek2005Models" role="doc-biblioref">2005</a>)</span></td>
</tr>
<tr class="even">
<td>Impulsiveness</td>
<td style="text-align: center;"><span class="math inline">I</span></td>
<td style="text-align: center;">iu</td>
<td><span class="citation" data-cites="Sottek2005Models">Sottek and Genuit (<a href="#ref-Sottek2005Models" role="doc-biblioref">2005</a>)</span></td>
</tr>
<tr class="odd">
<td>Fluctuation Strength</td>
<td style="text-align: center;"><span class="math inline">FS</span></td>
<td style="text-align: center;">vacil</td>
<td><span class="citation" data-cites="Sottek2005Models">Sottek and Genuit (<a href="#ref-Sottek2005Models" role="doc-biblioref">2005</a>)</span></td>
</tr>
<tr class="even">
<td>Tonality</td>
<td style="text-align: center;"><span class="math inline">T</span></td>
<td style="text-align: center;">tuHMS</td>
<td><span class="citation" data-cites="Sottek2005Models">Sottek and Genuit (<a href="#ref-Sottek2005Models" role="doc-biblioref">2005</a>)</span></td>
</tr>
<tr class="odd">
<td>Psychoacoustic Annoyance</td>
<td style="text-align: center;"><span class="math inline">PA</span></td>
<td style="text-align: center;">–</td>
<td><span class="citation" data-cites="Zwicker2007Psychoacoustics">Zwicker and Fastl (<a href="#ref-Zwicker2007Psychoacoustics" role="doc-biblioref">2007</a>)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">L_{Aeq}</span></td>
<td style="text-align: center;"><span class="math inline">L_{Aeq}</span></td>
<td style="text-align: center;">dB</td>
<td><span class="citation" data-cites="IEC2013Electroacoustics">IEC 61672-1:2013 (<a href="#ref-IEC2013Electroacoustics" role="doc-biblioref">2013</a>)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">L_{A10}-L_{A90}</span></td>
<td style="text-align: center;"><span class="math inline">L_{A10}-L_{A90}</span></td>
<td style="text-align: center;">dB</td>
<td><span class="citation" data-cites="ISO2016Acoustics">ISO 1996-1:2016 (<a href="#ref-ISO2016Acoustics" role="doc-biblioref">2016</a>)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">L_{Ceq}-L_{Aeq}</span></td>
<td style="text-align: center;"><span class="math inline">L_{Ceq}-L_{Aeq}</span></td>
<td style="text-align: center;">dB</td>
<td><span class="citation" data-cites="ISO2016Acoustics">ISO 1996-1:2016 (<a href="#ref-ISO2016Acoustics" role="doc-biblioref">2016</a>)</span></td>
</tr>
<tr class="odd">
<td>Relative Approach</td>
<td style="text-align: center;"><span class="math inline">RA</span></td>
<td style="text-align: center;">cPA</td>
<td><span class="citation" data-cites="Sottek2005Models">Sottek and Genuit (<a href="#ref-Sottek2005Models" role="doc-biblioref">2005</a>)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-corr" class="quarto-xref">Table&nbsp;2</a> shows the Pearson correlation coefficient between each of the candidate acoustic features and the outcome pleasantness and eventfulness. As all variables considered are continuous, and the eventual model is linear, the Pearson coefficient is chosen as a measure of the strength of the linear relationship between two continuous variables. For ISOPleasant (<em>ISOPl</em>), we can perhaps see three tiers of correlations: the more highly correlated tier <span class="math inline">(|r| &gt; 0.28)</span> consists of Relative Approach (<span class="math inline">RA</span>), <span class="math inline">L_{Aeq}</span>, Roughness (<span class="math inline">R</span>), Loudness (<span class="math inline">N_5</span>), and Psychoacoustic Annoyance (<span class="math inline">PA</span>); the low correlation tier consists of <span class="math inline">L_{A10}-L_{A90}</span>, Tonality (<span class="math inline">T</span>), and Fluctuation Strength (<span class="math inline">FS</span>); while <span class="math inline">L_{Ceq}-L_{Aeq}</span>, Impulsiveness (<span class="math inline">I</span>), and Sharpness (<span class="math inline">S</span>) show no correlation. For ISOEventful (<em>ISOEv</em>), these tiers are: <span class="math inline">RA</span>, <span class="math inline">L_{Aeq}</span>, <span class="math inline">T</span>, <span class="math inline">R</span>, and <span class="math inline">N_5</span> comprise the most correlated tier <span class="math inline">(|r| &gt; 0.30)</span>; <span class="math inline">L_{Ceq}-L_{Aeq}</span>, <span class="math inline">L_{A10}-L_{A90}</span>, <span class="math inline">FS</span>, and <span class="math inline">PA</span> show low correlations; <span class="math inline">I</span> and <span class="math inline">S</span> show no correlation.</p>
<div id="tbl-corr" class="striped hover quarto-float quarto-figure quarto-figure-center anchored" data-fontsize="10pt">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Pearson correlation coefficients between candidate acoustic features and ISOPleasant and ISOEventful across all 13 locations. Only statistically significant (<span class="math inline">p &lt; 0.01</span>) coefficients are shown.
</figcaption>
<div aria-describedby="tbl-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><strong>Parameter</strong></th>
<th style="text-align: center;"><strong>ISOPl</strong></th>
<th style="text-align: center;"><strong>ISOEv</strong></th>
<th style="text-align: center;"><strong><span class="math inline">PA</span></strong></th>
<th style="text-align: center;"><strong><span class="math inline">N_5</span></strong></th>
<th style="text-align: center;"><strong><span class="math inline">S</span></strong></th>
<th style="text-align: center;"><strong><span class="math inline">R</span></strong></th>
<th style="text-align: center;"><strong><span class="math inline">I</span></strong></th>
<th style="text-align: center;"><strong><span class="math inline">FS</span></strong></th>
<th style="text-align: center;"><strong><span class="math inline">T</span></strong></th>
<th style="text-align: center;"><strong><span class="math inline">L_{Aeq}</span></strong></th>
<th style="text-align: center;"><strong><span class="math inline">L_{A10}-L_{A90}</span></strong></th>
<th style="text-align: center;"><strong><span class="math inline">L_{Ceq}-L_{Aeq}</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">ISOPleasant</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">ISOEventful</td>
<td style="text-align: center;">-0.24</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">PA</span></td>
<td style="text-align: center;">-0.28</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">N_5</span></td>
<td style="text-align: center;">-0.37</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">S</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">R</span></td>
<td style="text-align: center;">-0.36</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">I</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-0.10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-0.37</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">FS</span></td>
<td style="text-align: center;">-0.11</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">T</span></td>
<td style="text-align: center;">-0.21</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">L_{Aeq}</span></td>
<td style="text-align: center;">-0.34</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">-0.09</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">L_{A10}-L_{A90}</span></td>
<td style="text-align: center;">-0.18</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">-0.20</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">L_{Ceq}-L_{Aeq}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-0.20</td>
<td style="text-align: center;">-0.49</td>
<td style="text-align: center;">-0.49</td>
<td style="text-align: center;">-0.54</td>
<td style="text-align: center;">-0.31</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-0.27</td>
<td style="text-align: center;">-0.28</td>
<td style="text-align: center;">-0.61</td>
<td style="text-align: center;">-0.22</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">RA</span></td>
<td style="text-align: center;">-0.34</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">-0.14</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Among the inter-correlations for the psychoacoustic metrics considered for inclusion as input features, we can see several very highly correlated features (i.e.&nbsp;<span class="math inline">&gt;0.9</span>). As expected, <span class="math inline">PA</span>, <span class="math inline">L_{Aeq}</span>, and <span class="math inline">N_5</span> are highly correlated, meaning that careful consideration is paid to these features to ensure they do not contribute to multicollinearity in the final model.</p>
</section>
</section>
<section id="modelling" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="modelling"><span class="header-section-number">2.2</span> Modelling</h3>
<p>Two linear multi-level models (MLM) were computed to predict: 1) ISOPleasant, and 2) ISOEventful. These models are trained on the 2019 data only, then applied to the acoustic data collected during the 2020 lockdowns, the results of which are reported in <a href="#sec-application" class="quarto-xref">Section&nbsp;3.2.3</a>. The inherent grouped structure of the SSID database necessitates a modeling and analysis approach which considers the differing relationships between the objective acoustic features and the soundscape’s perceived affective quality ratings across the various locations and contexts. The individual-level of the models is made up of the acoustic features calculated from the binaural recordings made during each respondent’s survey period, while the group-level includes the categorical <code>LocationID</code> variable indicating the location in which the survey was taken, acting as a non-auditory contextual factor.</p>
<p>A separate backwards-step feature selection was performed for each of the outcome models in order to identify the minimal feature set to be used for predicting each outcome. In this feature selection process, an initial model containing all of the candidate features was fit. Each feature was then removed from the model one at a time, then the best-performing model is selected and the procedure continues step-wise until no improvement is seen by removing more features. This process is carried out first on the location-level features (including the potential to remove all features including LocationID, resulting in a ‘flat’ or standard multivariate linear regression model), then on the individual-level features. The performance criterion used for this process was the Akaike Information Criterion (AIC) <span class="citation" data-cites="Akaike1974new">(<a href="#ref-Akaike1974new" role="doc-biblioref">Akaike 1974</a>)</span>. To check for multicollinearity among the selected features, the variance inflation factor (VIF) was calculated and a threshold of VIF &lt;5 was set. Any features which remained after the backwards stepwise selection and which exceeded this threshold were investigated and removed if they were highly collinear with the other features.</p>
<p>All of the input features are numeric values, in the units described above. Before conducting feature selection, the input features are z-scaled to enable proper comparison of their effect sizes. After the feature selection, the scaled coefficients are used in the text when reporting the final fitted models to facilitate discussion and comparison between the features. The unscaled model coefficients are reported in <strong>?@sec-appmod</strong> to enable the models to be applied to new data. In order to properly assess the predictive performance of the model, an 80/20 train-test split with a balanced shuffle across LocationIDs was used. The z-scaling and feature selection was performed on the training set only, in order to prevent data leakage. To score the performance of the model on the training and testing sets, we use the mean absolute error (MAE), which is in the scale of the response feature - for ISOPleasant this means our response can range from -1 to +1. However, since the end-goal of the model is to predict the soundscape assessment of the location as a whole, rather than the individual responses, we also assess the performance of the model in predicting the average response in each location. To do this, the mean response value for each location is calculated, and the <span class="math inline">R^2</span> accuracy across LocationIDs is reported for both the training and testing sets.</p>
<p>The model fitting and feature selection was performed using the <code>step</code> function from <code>lmerTest</code> (v3.1.3) <span class="citation" data-cites="Kuznetsova2017lmerTest">(<a href="#ref-Kuznetsova2017lmerTest" role="doc-biblioref">Kuznetsova, Brockhoff, and Christensen 2017</a>)</span> in R statistical software (v4.0.3) <span class="citation" data-cites="RCT2018R">(<a href="#ref-RCT2018R" role="doc-biblioref">R Core Team 2018</a>)</span>. The summaries and plots were created using the <code>sjPlot</code> package (v2.8.6) <span class="citation" data-cites="Luedecke2021sjPlot">(<a href="#ref-Luedecke2021sjPlot" role="doc-biblioref">Lüdecke 2021</a>)</span> and <code>seaborn</code> (v0.11.1) <span class="citation" data-cites="Waskom2021seaborn">(<a href="#ref-Waskom2021seaborn" role="doc-biblioref">Waskom 2021</a>)</span>.</p>
</section>
<section id="online-survey" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="online-survey"><span class="header-section-number">2.3</span> Online survey</h3>
<p>An online listening test was conducted using the Gorilla Experiment Builder<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <span class="citation" data-cites="AnwylIrvine2019Gorilla">(<a href="#ref-AnwylIrvine2019Gorilla" role="doc-biblioref">Anwyl-Irvine et al. 2019</a>)</span>. The participants were exposed to a random selection of 78 binaural recordings (39 from 2019 and 39 from 2020, 6 recordings per each location). Each participant had the option to evaluate either 1 or 2 sets of 6 recordings randomly assigned between 13 stimuli sets. Mp3 files, converted at 256 kBps were used due to the requirements of the Gorilla platform.</p>
<p>No visual stimuli were used in the experiment. The experiment consisted of: 1) an initial exercise to enhance chances of participants complying to the instructions and wearing headphones; 2) a training set using two randomly chosen binaural recordings (then not used in the main task) from the dataset; 3) a soundscape characterization questionnaire starting with an open-ended question about perceived sound sources and featuring the same questions as the one used in situ, looking into the perceived affective quality of the soundscape and the perceived sound source dominance of the following four types: traffic noise, other noise, human sounds and natural sounds; 4) a questionnaire on the basic demographic factors. The questionnaire used in the Part 3 of the online experiment is reported in Appendix A.</p>
<p>Having in mind the remote nature of the study and to ensure a minimum level of robustness for reliable sound source recognition, an initial exercise was performed consisting of a headphone screening test <span class="citation" data-cites="Woods2017Headphone">(<a href="#ref-Woods2017Headphone" role="doc-biblioref">Woods et al. 2017</a>)</span> and a headphone reproduction level adjustment test <span class="citation" data-cites="Gontier2019Estimation">(<a href="#ref-Gontier2019Estimation" role="doc-biblioref">Gontier et al. 2019</a>)</span>. The level adjustment was performed using an eleven-second-long pink noise sample matched to the lowest and the highest <span class="math inline">L_{A90}</span> values from the experimental set. Participants were asked to adjust their listening level to clearly hear the quieter sample while keeping the level low enough, so they don’t find the louder sample disturbing. The headphones screening test followed, featuring a stereo signal of one-second-long 100 Hz sine tone, generated with Izotope RX 6 application, played at a 3 dB difference where one of the equally loud pairs had its phase inverted. A 100 Hz sine was used because the pilot tests revealed the 200 Hz sine tone proposed by <span class="citation" data-cites="Woods2017Headphone">Woods et al. (<a href="#ref-Woods2017Headphone" role="doc-biblioref">2017</a>)</span> created a higher uncertainty varying across different laptop models and would likely contribute to the chances of a participant fooling the test. It was expected that participants using speakers would not be able to either hear the sine wave or would be fooled by the inverted phase effect and therefore not able to pass the trials, unless they were indeed using headphones. The participant needed to recognize the quietest of the 3 samples in a trial of 6 attempts. Only participants correctly answering 5 or more out of 6 trials were allowed to proceed with the experiment. Participants were asked not to change their audio output settings during the rest of the experiment. (This was introduced to ensure that a participant is using a headphones playback system which allows a listener to clearly recognize a 3 dB difference at 100 Hz as a proxy for sufficient audio quality playback.)</p>
<p>However, after the initial data collection, questions were raised as to how the playback loudness impacts ecological validity as it relates to the perceived affective quality of the soundscape. Given this concern, the PAQ responses from the online surveys were not included in further data analysis. Sound source identification is not considered to suffer the same validity concerns as this is not directly dependent on absolute playback level and requires only that the participant can clearly hear what is present. The purpose of the calibration procedure described above was to ensure the participant could clearly hear the softest samples used.</p>
<p>Online questionnaire data was collected between the 9th of June and the 9th of August 2020. Within the Gorilla Experiment Builder, a total of 250 attempts to complete the experiment were recorded, where 165 participants were excluded either on the basis of not passing the headphones screening (<span class="math inline">N=79</span>) or for not completing the experiment, usually before engaging into the screening (<span class="math inline">N=83</span>). Out of a total of 88 participants who completed the test, 2 participants were excluded as outliers as they provided uniform answers across all the questions and commented on not being able to properly hear the stimuli, despite their successful completion of the training tests. The participants of the online experiment were of mean age 32.42, 45.1% male, 54.9% female.</p>
<p><a href="#fig-framework" class="quarto-xref">Figure&nbsp;1</a> illustrates and summarizes the framework and sections described above.</p>
<div id="fig-framework" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figure1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The study flowchart indicating the data collection, analysis, modeling, and discussion throughout the study. The subsections in the text to which each box refers are indicated in italics.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="results" class="level2 page-columns page-full" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="results"><span class="header-section-number">3</span> Results</h2>
<p>The results of the onsite surveys, online experiment, and the model development are reported here. They are reported following the structure of the ISO/TS 12913 series, revealing the perceived sound source dominance, key perceptual attributes (ISOPleasant and ISOEventful) and the lockdown-related changes.</p>
<section id="perceived-sound-source-dominance" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="perceived-sound-source-dominance"><span class="header-section-number">3.1</span> Perceived sound source dominance</h3>
<section id="sound-source-composition-per-location" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="sound-source-composition-per-location"><span class="header-section-number">3.1.1</span> 2019 sound source composition per location</h4>
<p>Questionnaire data was collected in English, Italian, and Spanish in both cities. The respective questionnaires can be found in the supplementary files and <span class="citation" data-cites="Mitchell2020Soundscape">Mitchell et al. (<a href="#ref-Mitchell2020Soundscape" role="doc-biblioref">2020</a>)</span>. Data presented here was aggregated per LocationID.</p>
<p>According to the highest scored mean value of the dominant sound source type, as shown in <a href="#fig-barchart" class="quarto-xref">Figure&nbsp;2</a>, the locations can be grouped into: natural sounds dominated (RegentsParkJapan, RegentsParkFields, RussellSq), human sounds dominated (SanMarco, TateModern, StPaulsRow, StPaulsCross, MonumentoGaribaldi), noise (traffic and other noise) sounds dominated (CamdenTown, EustonTap, TorringtonSq, PancrasLock). Traffic noise and Other noise have been combined here, and for the rest of the discussion, as these responses are highly correlated within this dataset and it is not helpful to consider them separately for this analysis. This follows the alternative sound source labels given in Figure C.3 of <span class="citation" data-cites="ISO12913Part2">ISO/TS 12913-2:2018 (<a href="#ref-ISO12913Part2" role="doc-biblioref">2018</a>)</span> which combines Traffic and Other Noise. Finally, MarchmontGarden is unique in that all sound source types are assessed as being nearly equally present, with only 0.2 separating the least present (Other noise, 2.5) and the most present (Traffic noise, 2.7).</p>
<div id="fig-barchart" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-barchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figure2.jpg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-barchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: (Color online) Mean response per Location ID for the perceived dominance of the sound source types, for the 2019 on-site campaign. The values represent the mean response of all participants in each location to the question “To what extent do you presently hear the following four types of sounds?”. Response values range from [1] Not at all to [5] Dominates completely.
</figcaption>
</figure>
</div>
</section>
<section id="overall-change-in-the-perceived-sound-source-dominance-during-lockdown" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="overall-change-in-the-perceived-sound-source-dominance-during-lockdown"><span class="header-section-number">3.1.2</span> Overall change in the perceived sound source dominance during lockdown</h4>
<p>1803 words describing the sound sources present in the 2019 recordings and 1395 words related to the 2020 recordings were input by participants in response to the open-ended question Q1 (see Appendix A). The frequency of occurrence, generated using the WordClouds web app, is shown in the <a href="#fig-wordclo" class="quarto-xref">Figure&nbsp;3</a>, for the 2019 and the 2020 sets respectively. The most frequent words from both 2019 and 2020 groups are: noise, car/traffic, bird/birds, talk/voice and (foot)steps.</p>
<div id="fig-wordclo" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wordclo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="Figure3a.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="Figure3b.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wordclo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: A graphic illustrating the frequency of occurrence of the sound sources reported by the participants of the online study across all locations, shown for recordings from the 2019 (above) and 2020 (below).
</figcaption>
</figure>
</div>
<p>The results from the listening tests deployed online were analyzed using the SPSS Statistics v. 25. Levene’s test for equality of variances resulted in highly statistically significant values for all 4 sound sources investigated (less than 0.001). Therefore, a Mann-Whitney U-test test was used as a non-parametric equivalent to the T-test to investigate the change in the perceived dominance of the four sound source types <span class="citation" data-cites="McKnight2010Mann">(<a href="#ref-McKnight2010Mann" role="doc-biblioref">McKnight and Najab 2010</a>)</span>. The results for human sounds indicated that the perceived dominance was greater for the 2019 sample (M=3.82), than for the 2020 sample (M=2.62), U=41,656, p &lt; 0.001. The results for natural sounds indicated the perceived dominance increased from 2019 (M=2.00) to 2020 (M=2.54), U=63,797, p &lt; 0.001. However, the differences for the noise sources (traffic and other) were not statistically significant. The result of these changes is that while Human sounds were the clearly dominant source across the whole dataset in 2019, in 2020 the sound sources are, on average, much more evenly balanced. No single sound source category was identified as frequent across the 2020 dataset.</p>
<div id="tbl-source" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-source-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Mean values and standard deviation for the perceived dominance of sound sources (rated from 1 - 5), assessed via an online survey.
</figcaption>
<div aria-describedby="tbl-source-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 12%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 24%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Sound source type</th>
<th style="text-align: center;">Campaign</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Standard deviation</th>
<th style="text-align: center;">Standard error mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Traffic</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">422</td>
<td style="text-align: center;">2.51</td>
<td style="text-align: center;">1.369</td>
<td style="text-align: center;">0.067</td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">383</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">1.525</td>
<td style="text-align: center;">0.078</td>
</tr>
<tr class="odd">
<td>Other</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">422</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">1.182</td>
<td style="text-align: center;">0.058</td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">382</td>
<td style="text-align: center;">2.23</td>
<td style="text-align: center;">1.333</td>
<td style="text-align: center;">0.068</td>
</tr>
<tr class="odd">
<td>Human</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">423</td>
<td style="text-align: center;">3.82</td>
<td style="text-align: center;">1.143</td>
<td style="text-align: center;">0.056</td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">382</td>
<td style="text-align: center;">2.62</td>
<td style="text-align: center;">1.346</td>
<td style="text-align: center;">0.069</td>
</tr>
<tr class="odd">
<td>Natural</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">424</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">1.307</td>
<td style="text-align: center;">0.063</td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">380</td>
<td style="text-align: center;">2.54</td>
<td style="text-align: center;">1.441</td>
<td style="text-align: center;">0.074</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="model-selection-performance-and-application" class="level3 page-columns page-full" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="model-selection-performance-and-application"><span class="header-section-number">3.2</span> Model selection, performance, and application</h3>
<div id="tbl-model" class="striped hover column-page smaller quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Scaled linear regression models of ISOPleasant and ISOEventful for 13 locations in London and Venice. ISOPleasant model structure: Random slope, random intercept multi-level model. ISOEventful model structure: Multi-variate linear regression.
</figcaption>
<div aria-describedby="tbl-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover smaller caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 28%">
<col style="width: 20%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 9%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>ISOPleasant</th>
<th></th>
<th></th>
<th>ISOEventful</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Predictors</td>
<td>Estimates</td>
<td>Confidence Interval (CI)</td>
<td>p</td>
<td>Estimates</td>
<td>CI</td>
<td>p</td>
</tr>
<tr class="even">
<td>(Intercept)</td>
<td>0.24</td>
<td>0.15–0.33</td>
<td>&lt;0.001</td>
<td>0.14</td>
<td>0.12–0.16</td>
<td>&lt;0.001</td>
</tr>
<tr class="odd">
<td>N5</td>
<td>−0.06</td>
<td>−0.10–0.02</td>
<td>&lt;0.001</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>S</td>
<td></td>
<td></td>
<td></td>
<td>−0.08</td>
<td>−0.11–0.06</td>
<td>&lt;0.001</td>
</tr>
<tr class="odd">
<td>FS</td>
<td></td>
<td></td>
<td></td>
<td>−0.02</td>
<td>−0.05–0.00</td>
<td>0.033</td>
</tr>
<tr class="even">
<td>T</td>
<td></td>
<td></td>
<td></td>
<td>0.04</td>
<td>0.01–0.07</td>
<td>0.002</td>
</tr>
<tr class="odd">
<td><span class="math inline">L_{Aeq}</span></td>
<td></td>
<td></td>
<td></td>
<td>0.14</td>
<td>0.11–0.17</td>
<td>&lt;0.001</td>
</tr>
<tr class="even">
<td><span class="math inline">L_{Ceq}-L_{Aeq}</span></td>
<td></td>
<td></td>
<td></td>
<td>−0.03</td>
<td>−0.05–0.00</td>
<td>0.052</td>
</tr>
<tr class="odd">
<td><strong>Random effects</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><span class="math inline">\sigma^2</span></td>
<td>0.11</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><span class="math inline">\tau_{00}</span></td>
<td><span class="math inline">0.03_{LocationID}</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><span class="math inline">\tau_{11}</span></td>
<td><span class="math inline">0.02_{LocationID.L_{Aeq}}</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td><span class="math inline">0.00_{LocationID.L_{A10}-L_{A90}}</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">0.00_{LocationID.L_{Ceq}-L_{Aeq}}</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>ICC</td>
<td>0.90</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>N</td>
<td><span class="math inline">13_{LocationID}</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Observations</td>
<td>914</td>
<td></td>
<td></td>
<td>914</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>MAE train, test</td>
<td>0.258</td>
<td>0.259</td>
<td></td>
<td>0.233</td>
<td>0.231</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="isopleasant-model-selected" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="isopleasant-model-selected"><span class="header-section-number">3.2.1</span> ISOPleasant model selected</h4>
<p>Following the feature selection, the ISOPleasant model (given in <a href="#tbl-model" class="quarto-xref">Table&nbsp;4</a>) has <span class="math inline">N_5</span> as the fixed effect with a scaled coefficient of -0.06, and <span class="math inline">L_{Aeq}</span>, <span class="math inline">L_{A10}-L_{A90}</span>, and <span class="math inline">L_{Ceq}-L_{Aeq}</span> as coefficients which vary depending on the LocationID. The training and testing MAE are very similar, indicating that the model is neither over- nor under-fitting to the training data (<span class="math inline">MAE_{train} = 0.258</span>; <span class="math inline">MAE_{test} = 0.259</span>). The model performs very well at predicting the average soundscape assessment of the locations (<span class="math inline">R^2_{train} = 0.998</span>; <span class="math inline">R^2_{test} = 0.85</span>).</p>
<p>The high intraclass correlation (<span class="math inline">ICC = 0.90</span>) demonstrates that the location-level effects are highly important in predicting the pleasantness dimension. Within this random-intercept random-slope model structure, these effects include both the specific context of the location (i.e.&nbsp;the LocationID factor), but also the <span class="math inline">L_{Aeq}</span>, <span class="math inline">L_{A10}-L_{A90}</span>, and <span class="math inline">L_{Ceq}-L_{Aeq}</span> features whose effects vary across locations. These slopes are given in <a href="#fig-random" class="quarto-xref">Figure&nbsp;4</a>. This point highlights the need to consider how the context of a location will influence the relationship between the acoustic features and the perceived pleasantness.</p>
<div id="fig-random" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-random-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figure4.jpg" class="img-fluid figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-random-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: (Color online) Location-level scaled coefficients for the ISOPleasant model.
</figcaption>
</figure>
</div>
</section>
<section id="isoeventful-model-selected" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="isoeventful-model-selected"><span class="header-section-number">3.2.2</span> ISOEventful model selected</h4>
<p>Through the group-level feature selection, all of the group-level coefficients were removed, including the LocationID factor itself. Therefore the final ISOEventful model is a ‘flat’ multi-variate linear regression model, rather than a multi-level model. The ISOEventful model is a linear combination of S, FS, T, <span class="math inline">L_{Aeq}</span>, and <span class="math inline">L_{Ceq}-L_{Aeq}</span>. The training and testing MAE are very similar, indicating that the model is not over-fit to the training data (<span class="math inline">MAE_{train} = 0.233</span>; <span class="math inline">MAE_{test} = 0.231</span>). The model performs slightly worse than the ISOPleasant at predicting the mean location responses, but still performs well (<span class="math inline">R^2_{train} = 0.873</span>; <span class="math inline">R^2_{test} = 0.715</span>).</p>
</section>
<section id="sec-application" class="level4" data-number="3.2.3">
<h4 data-number="3.2.3" class="anchored" data-anchor-id="sec-application"><span class="header-section-number">3.2.3</span> Application to lockdown data</h4>
<div id="fig-locations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-locations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figure5.jpg" class="img-fluid figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-locations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: (Color online) Soundscape circumplex coordinates for (a) the mean ISOPleasant and ISOEventful responses for each location; and (b) the mean predicted responses based on recordings made during the lockdown and the change in the location’s placement in the circumplex. In (b) the marker outline is shown for the 2019 location, red arrows indicate the change in the location’s coordinates.
</figcaption>
</figure>
</div>
<p>Once the two models were built and assessed, they were then applied to the lockdown recording data in order to predict the new soundscape ISO coordinates. <a href="#fig-locations" class="quarto-xref">Figure&nbsp;5</a> (a) shows the pre-lockdown ISO coordinates for each location and <a href="#fig-locations" class="quarto-xref">Figure&nbsp;5</a> (b) shows how the soundscapes are predicted to have been assessed during the lockdown period. As in the model assessment process, the predicted responses are calculated for each recording individually, then the mean for each location is calculated and plotted on the circumplex.</p>
<p>In 2019 the majority of locations in the dataset fall within the ‘vibrant’ quadrant of the circumplex, particularly those which are primarily dominated by human activity (e.g.&nbsp;San Marco, Tate Modern). Camden Town and Euston Tap, which are both in general visually and acoustically dominated by traffic, are the only two to be rated as ‘chaotic’, while no locations are overall considered to be ‘monotonous’. During the 2020 lockdown, there is general positive move along the ‘pleasant’ dimension and general negative move along the ‘eventful’ dimension, but several different patterns of movement can be noted. These are investigated further in the Discussion section below.</p>
</section>
</section>
</section>
<section id="discussion" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="discussion"><span class="header-section-number">4</span> Discussion</h2>
<section id="interpretation-of-the-results" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="interpretation-of-the-results"><span class="header-section-number">4.1</span> Interpretation of the results</h3>
<p>To interpret the results addressing the RQ1 and RQ2, it is necessary to separately look into the overall change in sound source composition, and the change in the affective quality of soundscapes per location.</p>
<section id="change-in-the-sound-source-composition" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="change-in-the-sound-source-composition"><span class="header-section-number">4.1.1</span> Change in the sound source composition</h4>
<p>The open-ended question about sound sources in the online survey did not reveal a change in sound source types but rather confirmed that all types were still present in both conditions. The sound source composition question taken from the Method A of the <span class="citation" data-cites="ISO12913Part2">ISO/TS 12913-2:2018 (<a href="#ref-ISO12913Part2" role="doc-biblioref">2018</a>)</span> revealed a statistically significant reduction in human sound sources and a significant increase in the perceived dominance of natural sound sources.</p>
<p>The most frequent sound sources detected from the open-ended question correspond to the main four sound source types investigated, which indicated that all types remained present in the lockdown condition (at all the locations). While traffic intensity might have gone down, where the results of the Mann-Whitney U-test were inconclusive, but supported by the psychoacoustic measurements <span class="citation" data-cites="Aletta2020Assessing">(<a href="#ref-Aletta2020Assessing" role="doc-biblioref">Aletta et al. 2020</a>)</span>, traffic-related sound sources were still clearly present.</p>
<p>The sound source composition of an outdoor acoustic environment is extremely complex. Removing one component, such as human sounds, has implications on the whole <span class="citation" data-cites="Gordo2021Rapid">(<a href="#ref-Gordo2021Rapid" role="doc-biblioref">Gordo et al. 2021</a>)</span>. Testing the effects of this in situ is not straightforward and interpreting this study in line with ‘what is the impact of human sounds’ must be taken within the broader context of the range of conditions which changed within the acoustic environment. However, looking at the overarching picture, the lockdown condition was a useful and unique case study to understand the impact which human activities – and the human sound source type in particular – can have on soundscape perception of urban open spaces.</p>
</section>
<section id="predicted-relative-changes-in-soundscapes-due-to-covid-19-restrictions" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="predicted-relative-changes-in-soundscapes-due-to-covid-19-restrictions"><span class="header-section-number">4.1.2</span> Predicted relative changes in soundscapes due to COVID-19 restrictions</h4>
<div id="fig-vectors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vectors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figure6.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vectors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: (Color online) The relative change in soundscape perception in the circumplex due to the COVID-19 lockdowns as predicted by the models, represented as vectors centered on the origin. <em>The lawn-works dominated session is shown separately as MonumentoGaribaldi</em> with a gray arrow to indicate that this is distinct from the effects of the lockdown changes.
</figcaption>
</figure>
</div>
<p>In order to interpret how the change of the acoustic environment at the locations examined would have been perceived, and to answer RQ2, relative change vectors within the circumplex space are shown in Figure . This clearly shows a few different patterns of soundscape change due to the effects of the 2020 lockdown. These can be further looked into depending on the magnitude and direction of change, shifts between quadrants shown in <a href="#fig-locations" class="quarto-xref">Figure&nbsp;5</a>, and the sound source composition. The discussion below is organized according to groups of locations which show similar behaviors in the predicted magnitude and direction of change, or discusses a single location which is particularly notable.</p>
<section id="piazza-san-marco" class="level5" data-number="4.1.2.1">
<h5 data-number="4.1.2.1" class="anchored" data-anchor-id="piazza-san-marco"><span class="header-section-number">4.1.2.1</span> Piazza San Marco</h5>
<p>The largest change is seen in Piazza San Marco, with a predicted increase in pleasantness of 0.24 and a decrease in eventfulness of 0.44, enough to move the soundscape out of the ‘vibrant’ quadrant and into ‘calm’. This extreme change (relative to the rest of the locations) is exactly what would be expected given the unique context of the measurements taken in 2019 – the measurement campaign corresponded with Carnevale, a yearly festival which centers around the square. By contrast, due to the particularly strict measures imposed in Italy, during the lockdown measurement period, the square was almost entirely devoid of people. What is promising is that, without any of this contextual information about the presence or absence of people, our model is able to capture and reflect what may be considered a reasonable and expected direction and scale of change within the soundscape circumplex.</p>
</section>
<section id="locations-showing-an-increase-in-pleasantness" class="level5" data-number="4.1.2.2">
<h5 data-number="4.1.2.2" class="anchored" data-anchor-id="locations-showing-an-increase-in-pleasantness"><span class="header-section-number">4.1.2.2</span> Locations showing an increase in pleasantness</h5>
<p>The next locations of interest are those which, in the 2019 survey data, were rated as being dominated by traffic noise: Euston Tap, Camden Town, Torrington Square, and Pancras Lock. These are the only locations (besides San Marco) which show a predicted increase in pleasantness. Of these traffic-dominated spaces, the two which were most heavily dominated by traffic noise (Camden Town and Euston Tap) showed the most increase in pleasantness, with Torrington Square having slightly less of an increase. Pancras Lock, which was also rated as having high levels of both Human and Natural sounds shows only a modest improvement in pleasantness.</p>
</section>
<section id="locations-showing-a-decrease-in-pleasantness" class="level5" data-number="4.1.2.3">
<h5 data-number="4.1.2.3" class="anchored" data-anchor-id="locations-showing-a-decrease-in-pleasantness"><span class="header-section-number">4.1.2.3</span> Locations showing a decrease in pleasantness</h5>
<p>Among the locations which are predicted to experience a negative effect on pleasantness we see a mix of spaces which were assessed as being dominated by Human (St Pauls Cross and Tate Modern) and Natural (Regents Park Japan, Regents Park Fields, Russell Square) sounds before the lockdown. It is hard to discern a pattern of difference between these two groups, although it appears that the Human-dominated spaces saw a greater reduction in eventfulness, compared to the Natural-dominated spaces.</p>
<p>In general, we note that most of the spaces experience some degree of reduction in eventfulness. This pattern is particularly consistent with what would be expected from a reduction in human presence in these spaces <span class="citation" data-cites="Aletta2018Towards">(<a href="#ref-Aletta2018Towards" role="doc-biblioref">Aletta and Kang 2018</a>)</span>, as reflected by the observation that, in general, those spaces which had the most human sounds prior to the lockdown showed the greatest reduction in eventfulness during the lockdown. In particular, Tate Modern, Camden Town, and Torrington Square show the greatest reduction in eventfulness. This appears to be due to these locations showing the greatest reduction in overall <span class="math inline">L_{Aeq}</span> compared to other locations (8.1 dB, 5.2 dB, and 9.2 dB, respectively), with <span class="math inline">L_{Aeq}</span> being the most influential feature in the eventfulness model, as shown in <a href="#tbl-model" class="quarto-xref">Table&nbsp;4</a>. However, Russell Square also experienced a large decrease in <span class="math inline">L_{Aeq}</span> on average (10.5 dB) but does not show the same reduction in eventfulness. This appears due to the correspondingly large decrease in <span class="math inline">S</span> (1.17 acum) which is not seen at the 3 previously mentioned locations. Russell Square normally features a medium-sized jet fountain which was turned off during the lockdowns in 2020 and therefore it experienced a drop in the overall sound level, but an increase in the proportion of low frequency noise to high frequency noise reflected by a decrease in sharpness which, within the eventfulness model, effectively cancels out the impact of the reduction in <span class="math inline">L_{Aeq}</span>. While the overall sound level has an important impact, in order to determine the true impact a reduction in sound level may have, it must be taken in context with how the other aspects of the sound will also change.</p>
</section>
<section id="euston-tap" class="level5" data-number="4.1.2.4">
<h5 data-number="4.1.2.4" class="anchored" data-anchor-id="euston-tap"><span class="header-section-number">4.1.2.4</span> Euston Tap</h5>
<p>An unexpected result is that Euston Tap is predicted to experience an increase in eventfulness and it is unclear whether this accurately reflects the real experience people would have had in the space. Normally, Euston Tap is a mostly-outdoor drinking venue located at the entrance to London Euston Station and situated directly along a very busy central London road. During the 2020 survey, the researchers noted that the music and chatter of people from the pub was noticeably missing, but that the perceived reduction in road traffic was minimal. Based on the theory of vibrancy which would suggest it is driven by human presence and sounds <span class="citation" data-cites="Aletta2018Towards">(<a href="#ref-Aletta2018Towards" role="doc-biblioref">Aletta and Kang 2018</a>)</span>, we would not therefore expect a shift in the vibrant direction as indicated here. This discrepancy may reveal a weakness in the context-independent ISOEventful model, or it may in fact be indicating that, at certain thresholds of traffic noise, a reduction in level – and therefore a reduction in energetic masking – will allow other aspects of the sound to influence the perception.</p>
</section>
<section id="monumento-garibaldi" class="level5" data-number="4.1.2.5">
<h5 data-number="4.1.2.5" class="anchored" data-anchor-id="monumento-garibaldi"><span class="header-section-number">4.1.2.5</span> Monumento Garibaldi</h5>
<p>Finally, special attention should be paid to the results shown for Monumento Garibaldi, which in 2019 was perceived as a pleasant and slightly calm green space featuring a gravel walkway. During the first measurement session during the lockdown in 2020, the researcher noted that the soundscape was dominated by landscaping works, in particular noise from strimmers (or weed whackers). In order to gain a sample which was more representative of the impact of the lockdowns, the researcher returned another day to repeat the measurements without interference from the works.</p>
<p>To examine the impact of these two scenarios separately, the prediction model was fitted to the data from the two sessions independently and the session which was impacted by the landscaping works is shown in <a href="#fig-vectors" class="quarto-xref">Figure&nbsp;6</a> in gray and labeled MonumentoGaribaldi*, while the unaffected session is shown in red. In the latter case, the predicted change in soundscape as a result of the lockdown fits neatly into what would be expected and closely matches the predicted behavior of similar locations in London (i.e.&nbsp;Marchmont Garden and Russell Square). On the other hand, the session which was dominated by noise from the strimmers is predicted to have become much more chaotic, with a decrease in pleasantness of 0.16 and an increase in eventfulness of 0.27. This indicates that, although the model has no contextual information about the type of sound and in fact the training data never included sounds from similar equipment, just based on the psychoacoustic features of the sound it is able to reasonably predict the expected change in soundscape.</p>
</section>
<section id="general-notes" class="level5" data-number="4.1.2.6">
<h5 data-number="4.1.2.6" class="anchored" data-anchor-id="general-notes"><span class="header-section-number">4.1.2.6</span> General notes</h5>
<p>As a whole, the primary impact of the 2020 lockdowns on the soundscapes in London and Venice was an overall decrease in eventfulness. With the exception of Euston Tap, all of the sessions show some degree of reduction in eventfulness, reflecting the general decrease in sound levels and human sound sources across the locations. The impact of the lockdowns on pleasantness is more mixed and seems to be driven by the previous dominance of traffic noise in the space. However, it could also be noted that, while all locations experienced a reduction in sound level, those which are predicted to become more pleasant had an average <span class="math inline">L_{Aeq}</span> above 60 dB in 2019. By contrast, the locations which were predicted to experience a decrease in pleasantness generally had sound levels below 60 dBA in 2019. This may indicate that reductions in sound level can improve pleasantness when the sound level exceeds some threshold of around 60 - 65 dBA but are ineffective when sound levels are below this threshold. Similarly, <span class="citation" data-cites="Yang2005Acoustic">Yang and Kang (<a href="#ref-Yang2005Acoustic" role="doc-biblioref">2005</a>)</span> showed that, when the sound level is ‘lower than a certain value, say 70 dB’ there is no longer a significant improvement in the evaluation of acoustic comfort as the sound level reduces. It is unclear at this point where this threshold would lie for pleasantness/annoyance, how strict it may be, or how it is impacted by the sound source composition of the acoustic environment, therefore further research is needed in this area.</p>
</section>
</section>
<section id="model-selection-results" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="model-selection-results"><span class="header-section-number">4.1.3</span> Model selection results</h4>
<p>The most immediately interesting result of the model-building and feature selection process, answering to RQ3, is the apparent irrelevance of location context to the ISOEventful dimension. The multilevel model structure was chosen since the starting assumption was that soundscape perception is heavily influenced by contextual factors, such as expectations of the space and visual context <span class="citation" data-cites="Ricciardi2015Sound">(<a href="#ref-Ricciardi2015Sound" role="doc-biblioref">Ricciardi et al. 2015</a>)</span>. For this modeling, these factors can be considered as location-level latent variables at least partially accounted for by the inclusion of the LocationID as the second-level factor. While this assumption certainly held true for ISOPleasant, our results indicate that these types of contextual factors are not significant for ISOEventful, and do not affect the relationship between the acoustic features of the sound and the perception.</p>
<p>In particular this result may herald a shift in modeling approach for soundscapes – where previous methods, in both the soundscape and noise paradigms, have mostly focused on deriving acoustic models of annoyance (in other words have focused on the ISOPleasant dimension) perhaps they should instead consider the acoustic models as primarily describing the eventfulness dimension when considered in situ. In addition this study takes the approach of modeling responses at an individual level in order to derive the soundscape assessment of the location. Rather than either attempting to represent the predicted response of an individual person – which is less useful in this sort of practical application – or to base the model on average metrics of the location, the goal is instead to characterize the location itself, through the aggregated predicted responses of individuals. The authors believe this modeling approach better addresses the practical goal of predictive soundscape modeling and reflects the structure of the data collection.</p>
</section>
</section>
<section id="limitations-of-the-study" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="limitations-of-the-study"><span class="header-section-number">4.2</span> Limitations of the study</h3>
<p>The onsite sampling method was initially not intended as the ultimate characterization of a location’s soundscape but rather as a tool for model development. Therefore, the change observed does not necessarily represent the ground truth about the site’s soundscape, if such a thing exists. Further, the online listening tests took a relatively small but random sample from the available database and did not include any contextual information. This proved to be sufficient for the purpose of detecting a change in sound source composition, however the relatively small sample of recordings included in the online study does limit how representative they are of the location’s sound environment as a whole. Similarly, the surveys and recordings taken represent only a snapshot of the soundscape or sound environment for a short period in time. This is a flaw in most soundscape sampling methods presented both in the literature and in ISO/TS 12913-2. To truly be said to characterize the soundscape of a space, long-term monitoring and survey methods will need to be developed in order to capture the changing environmental and contextual conditions in the space. Models of the sort presented here, which are based on measurable quantities, could prove to be useful in this sort of longterm monitoring as they could take continuous inputs from sensors and generate the likely soundscape assessment over time.</p>
<p>The audio-visual interaction forms a key component in people’s perception of urban spaces. This consideration has been a strength of soundscape research and has been incorporated via the use of in situ data collection. However, the visual aspect, and in particular how the visual environment changed as a result of the lockdown condition, was not considered in this study, reducing the comprehensiveness of the model. This was due primarily to data collection limitations imposed by the lockdown restrictions which made it impractical to replicate the <span class="math inline">360^{\circ}</span> videos made during the 2019 sessions. Future work on comprehensive predictive soundscape models should strive to make use of this visual aspect within their considered features.</p>
<p>The limitation of the sound source categorization adopted from the ISO standard is that it may not be clear to a respondent in which category they would place community sounds like church bells and music. This may be particularly relevant for comparing the lockdown condition, as in particular the ringing of bells for worship varied in different contexts throughout the pandemic. Whether bells ceased entirely or were increased not only would have an impact on the sound environment, but the purposeful action behind the decision to ring bells may have changed to public’s relationship to and perception of the sound itself <span class="citation" data-cites="Parker2020Anthropause">(<a href="#ref-Parker2020Anthropause" role="doc-biblioref">Parker and Spennemann 2020</a>)</span>. The open ended question on sound sources, however, revealed the presence of church bells in both years. Unfortunately, this is a limitation of the sound source categories given by the ISO standard on which this questionnaire was based. A sensible update based on the findings and experiences reported here would be to combine the Traffic and Other Noise categories as separating them does not appear to provide additional information, and to include a new category which would in some way encapsulate the types of community sounds for which there is currently not a clear category.</p>
<p>Further, the lockdown condition is likely to cause distortions of the circumplex soundscape perception model. Therefore, it is important to acknowledge that all the predictions were made for the people with no experience of the pandemic and its psychological effects. Conceptually, this model captured the perceptual mapping (i.e.&nbsp;the relationship between the acoustic indicator inputs and the soundscape descriptor outputs) of people in 2019, but this perceptual mapping is likely to have been affected by the psychological and contextual impacts of the lockdown itself, independent of its changes on the sound environment. Future research might look into potential perception changes in the post-pandemic world.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">5</span> Conclusion</h2>
<p>This study demonstrates an application of predictive modeling to the field of soundscape studies. The model building results reveal that, within this dataset, an approach based on psychoacoustics can achieve <span class="math inline">R^2 = 0.85</span> for predicting the pleasantness of locations and <span class="math inline">R^2 = 0.715</span> for predicting the eventfulness. A modeling–focused method of this sort is a key component to the potential scalability of the soundscape approach to applications such as smart city sensing, urban planning, and cost-effective, sustainable design. To demonstrate the usefulness and feasibility of such an approach, we apply our predictive model to a unique case study in which traditional soundscape survey methods were impossible.</p>
<p>By applying this predictive model to recordings collected during the 2020 lockdown, the change in perception of the urban soundscapes is revealed. In general, soundscapes became less eventful, and those locations which were previously dominated by traffic noise became more pleasant. By contrast, previously human- and natural-dominated locations are in fact predicted to become less pleasant despite the decrease in sound levels. While all sound source categories remained present in both years, overall, in 2020 a decrease in human sounds’ dominance was observed together with an increase in the perceived dominance of natural sounds. Although these results are limited in that they represent one snapshot of the soundscape of the spaces, the success of the model in responding to new and disturbing sound events demonstrates its potential usefulness in long-term monitoring of urban soundscapes.</p>
</section>



<section id="references" class="level2 unnumbered">




</section>


<div id="quarto-appendix" class="default"><section id="acknowledgements" class="level2 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Acknowledgements</h2><div class="quarto-appendix-contents">

<p>This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (Grant Agreement No.&nbsp;740696, project title: Soundscape Indices – SSID). More information and related publications can be found at the CORDIS webpage of the project<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<p>The authors would like to thank Zhongzhe Li for conducting binaural recordings at Euston Square Gardens and Torrington Square in 2019. The authors would like to thank Meihui Ba, Nicolas Assiotis, Veronica Rugeles Allan, Yu Wang and Hua Su for their help in conducting on-site surveys during the spring and the autumn of 2019.</p>
<p>On-site study data were collected and managed using REDCap electronic data capture tools hosted at University College London (UCL).</p>
</div></section><section id="appendix-a-online-survey-questionnaire" class="level2 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Appendix A: Online survey questionnaire</h2><div class="quarto-appendix-contents">

<div id="tbl-gorilla" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-gorilla-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: Questionnaire deployed via the Gorilla Experiment Builder
</figcaption>
<div aria-describedby="tbl-gorilla-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 92%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Q1</strong></td>
<td><strong>While listening, please note any sound sources you can identify in this sound environment</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Q2</strong></td>
<td><strong>To what extent have you heard the following four types of sounds?</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td><strong>Traffic noise (e.g., cars, buses, trains, airplanes)</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td>Not at all / A little / Moderately / A lot / Dominates completely</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td><strong>Other noise (e.g., sirens, construction, industry, loading of goods)</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td>Not at all / A little / Moderately / A lot / Dominates completely</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td><strong>Sounds from human beings (e.g., conversation, laughter, children at play, footsteps)</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td>Not at all / A little / Moderately / A lot / Dominates completely</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td><strong>Natural sounds (e.g., singing birds, flowing water, wind in vegetation)</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td>Not at all / A little / Moderately / A lot / Dominates completely</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div></section><section id="sec-appmod" class="level2 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Appendix B: Model Results</h2><div class="quarto-appendix-contents">

<p><a href="#tbl-unscl" class="quarto-xref">Table&nbsp;6</a> presents the unscaled coefficients for the ISOPleasant and ISOEventful predictive models. The scaled coefficients are presented in the body of the text to facilitate comparisons between the various factors. However, we feel it is important to present unscaled coefficients such that these models could be implemented and compared for future work.</p>
<div id="tbl-unscl" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-unscl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: The unscaled linear regression models of ISOPleasant and ISOEventful for 13 locations in London and Venice. Statistically significant p-values are highlighted in bold.
</figcaption>
<div aria-describedby="tbl-unscl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 28%">
<col style="width: 20%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 9%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>ISOPleasant</th>
<th></th>
<th></th>
<th>ISOEventful</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Predictors</td>
<td>Estimates</td>
<td>Confidence Interval (CI)</td>
<td>p</td>
<td>Estimates</td>
<td>CI</td>
<td>p</td>
</tr>
<tr class="even">
<td>(Intercept)</td>
<td>0.38</td>
<td>0.28–0.50</td>
<td>&lt;0.001</td>
<td>-0.77</td>
<td>-1.05-0.48</td>
<td>&lt;0.001</td>
</tr>
<tr class="odd">
<td>N5</td>
<td>−0.01</td>
<td>−0.01–0.00</td>
<td>&lt;0.001</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>S</td>
<td></td>
<td></td>
<td></td>
<td>−0.17</td>
<td>−0.23–0.12</td>
<td>&lt;0.001</td>
</tr>
<tr class="odd">
<td>FS</td>
<td></td>
<td></td>
<td></td>
<td>−1.36</td>
<td>−2.61–0.11</td>
<td>0.033</td>
</tr>
<tr class="even">
<td>T</td>
<td></td>
<td></td>
<td></td>
<td>0.24</td>
<td>0.08–0.39</td>
<td>0.002</td>
</tr>
<tr class="odd">
<td><span class="math inline">L_{Aeq}</span></td>
<td></td>
<td></td>
<td></td>
<td>0.02</td>
<td>0.08–0.29</td>
<td>&lt;0.001</td>
</tr>
<tr class="even">
<td><span class="math inline">L_{Ceq}-L_{Aeq}</span></td>
<td></td>
<td></td>
<td></td>
<td>−0.01</td>
<td>−0.02–0.00</td>
<td>0.052</td>
</tr>
<tr class="odd">
<td><strong>Random effects</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><span class="math inline">\sigma^2</span></td>
<td>0.11</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><span class="math inline">\tau_{00}</span></td>
<td><span class="math inline">1.01_{LocationID}</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><span class="math inline">\tau_{11}</span></td>
<td><span class="math inline">0.00_{LocationID.L_{Aeq}}</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td><span class="math inline">0.00_{LocationID.L_{A10}-L_{A90}}</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">0.00_{LocationID.L_{Ceq}-L_{Aeq}}</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>ICC</td>
<td>0.90</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>N</td>
<td><span class="math inline">13_{LocationID}</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Observations</td>
<td>914</td>
<td></td>
<td></td>
<td>914</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-unsclRandom" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-unsclRandom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figure7.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-unsclRandom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: (Color online) The unscaled location-level coefficients for the ISOPleasant model.
</figcaption>
</figure>
</div>
</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Aiello2016Chatty" class="csl-entry" role="listitem">
Aiello, Luca Maria, Rossano Schifanella, Daniele Quercia, and Francesco Aletta. 2016. <span>“Chatty Maps: Constructing Sound Maps of Urban Areas from Social Media Data.”</span> <em>Royal Society Open Science</em> 3 (3): 150690. <a href="https://doi.org/10.1098/rsos.150690">https://doi.org/10.1098/rsos.150690</a>.
</div>
<div id="ref-Akaike1974new" class="csl-entry" role="listitem">
Akaike, H. 1974. <span>“A New Look at the Statistical Model Identification.”</span> <em><span>IEEE</span> Transactions on Automatic Control</em> 19 (6): 716–23. <a href="https://doi.org/10.1109/tac.1974.1100705">https://doi.org/10.1109/tac.1974.1100705</a>.
</div>
<div id="ref-Aletta2017Dimensions" class="csl-entry" role="listitem">
Aletta, Francesco, Östen Axelsson, and Jian Kang. 2017. <span>“<span class="nocase">Dimensions underlying the perceived similarity of acoustic environments</span>.”</span> <em>Frontiers in Psychology</em> 8 (July): 1–11. <a href="https://doi.org/10.3389/fpsyg.2017.01162">https://doi.org/10.3389/fpsyg.2017.01162</a>.
</div>
<div id="ref-Aletta2018Towards" class="csl-entry" role="listitem">
Aletta, Francesco, and Jian Kang. 2018. <span>“<span class="nocase">Towards an urban vibrancy model: A soundscape approach</span>.”</span> <em>International Journal of Environmental Research and Public Health</em> 15 (8): 1712. <a href="https://doi.org/10.3390/ijerph15081712">https://doi.org/10.3390/ijerph15081712</a>.
</div>
<div id="ref-Aletta2016Soundscape" class="csl-entry" role="listitem">
Aletta, Francesco, Jian Kang, and Östen Axelsson. 2016. <span>“<span class="nocase">Soundscape descriptors and a conceptual framework for developing predictive soundscape models</span>.”</span> <em>Landscape and Urban Planning</em> 149 (July): 65–74. <a href="https://doi.org/10.1016/j.landurbplan.2016.02.001">https://doi.org/10.1016/j.landurbplan.2016.02.001</a>.
</div>
<div id="ref-Aletta2020Assessing" class="csl-entry" role="listitem">
Aletta, Francesco, Tin Oberman, Andrew Mitchell, Huan Tong, and Jian Kang. 2020. <span>“<span class="nocase">Assessing the changing urban sound environment during the COVID-19 lockdown period using short-term acoustic measurements</span>.”</span> <em>Noise Mapping</em> 7 (1): 123–34. <a href="https://doi.org/10.1515/noise-2020-0011">https://doi.org/10.1515/noise-2020-0011</a>.
</div>
<div id="ref-AlsinaPages2021Changes" class="csl-entry" role="listitem">
Alsina-Pagès, Rosa Ma, Pau Bergadà, and Carme Martı́nez-Suquı́a. 2021. <span>“Changes in the Soundscape of <span>G</span>irona During the <span>COVID</span> Lockdown.”</span> <em>The Journal of the Acoustical Society of America</em> 149 (5): 3416–23. <a href="https://doi.org/10.1121/10.0004986">https://doi.org/10.1121/10.0004986</a>.
</div>
<div id="ref-AnwylIrvine2019Gorilla" class="csl-entry" role="listitem">
Anwyl-Irvine, Alexander L., Jessica Massonnié, Adam Flitton, Natasha Kirkham, and Jo K. Evershed. 2019. <span>“Gorilla in Our Midst: An Online Behavioral Experiment Builder.”</span> <em>Behavior Research Methods</em> 52 (1): 388–407. <a href="https://doi.org/10.3758/s13428-019-01237-x">https://doi.org/10.3758/s13428-019-01237-x</a>.
</div>
<div id="ref-Asensio2020Taxonomy" class="csl-entry" role="listitem">
Asensio, César, Pierre Aumond, Arnaud Can, Luis Gascó, Peter Lercher, Jean-Marc Wunderli, Catherine Lavandier, et al. 2020. <span>“A <span>Taxonomy</span> <span>Proposal</span> for the <span>Assessment</span> of the <span>Changes</span> in <span>Soundscape</span> <span>Resulting</span> from the <span>COVID</span>-19 <span>Lockdown</span>.”</span> <em>International Journal of Environmental Research and Public Health</em> 17 (12): 4205. <a href="https://doi.org/10.3390/ijerph17124205">https://doi.org/10.3390/ijerph17124205</a>.
</div>
<div id="ref-Asensio2020Changes" class="csl-entry" role="listitem">
Asensio, César, Ignacio Pavón, and Guillermo de Arcas. 2020. <span>“Changes in Noise Levels in the City of <span>Madrid</span> During <span>COVID</span>-19 Lockdown in 2020.”</span> <em>The Journal of the Acoustical Society of America</em> 148 (3): 1748–55. <a href="https://doi.org/10.1121/10.0002008">https://doi.org/10.1121/10.0002008</a>.
</div>
<div id="ref-Axelsson2010principal" class="csl-entry" role="listitem">
Axelsson, Östen, Mats E. Nilsson, and Birgitta Berglund. 2010. <span>“<span class="nocase">A principal components model of soundscape perception</span>.”</span> <em>The Journal of the Acoustical Society of America</em> 128 (5): 2836–46. <a href="https://doi.org/10.1121/1.3493436">https://doi.org/10.1121/1.3493436</a>.
</div>
<div id="ref-BonetSola2021Soundscape" class="csl-entry" role="listitem">
Bonet-Solà, Daniel, Carme Martı́nez-Suquı́a, Rosa Ma Alsina-Pagès, and Pau Bergadà. 2021. <span>“The Soundscape of the <span>COVID</span>-19 Lockdown: <span>B</span>arcelona Noise Monitoring Network Case Study.”</span> <em>International Journal of Environmental Research and Public Health</em> 18 (11): 5799. <a href="https://doi.org/10.3390/ijerph18115799">https://doi.org/10.3390/ijerph18115799</a>.
</div>
<div id="ref-Erfanian2021Psychological" class="csl-entry" role="listitem">
Erfanian, Mercede, Andrew Mitchell, Francesco Aletta, and Jian Kang. 2021. <span>“Psychological Well-Being and Demographic Factors Can Mediate Soundscape Pleasantness and Eventfulness: A Large Sample Study.”</span> <em>Journal of Environmental Psychology</em> 77 (October): 101660. <a href="https://doi.org/10.1016/j.jenvp.2021.101660">https://doi.org/10.1016/j.jenvp.2021.101660</a>.
</div>
<div id="ref-EuropeanUnion2002Directive" class="csl-entry" role="listitem">
European Union. 2002. <em><span class="nocase">Directive 2002/49/EC of the European Parliament and of the Council of 25 June 2002 relating to the assessment and management of environmental noise</span></em>.
</div>
<div id="ref-Gontier2019Estimation" class="csl-entry" role="listitem">
Gontier, Félix, Catherine Lavandier, Pierre Aumond, Mathieu Lagrange, and Jean-François Petiot. 2019. <span>“Estimation of the Perceived Time of Presence of Sources in Urban Acoustic Environments Using Deep Learning Techniques.”</span> <em>Acta Acustica United with Acustica</em> 105 (6): 1053–66. <a href="https://doi.org/10.3813/aaa.919384">https://doi.org/10.3813/aaa.919384</a>.
</div>
<div id="ref-Gordo2021Rapid" class="csl-entry" role="listitem">
Gordo, Oscar, Lluı́s Brotons, Sergi Herrando, and Gabriel Gargallo. 2021. <span>“Rapid Behavioural Response of Urban Birds to <span>COVID</span>-19 Lockdown.”</span> <em>Proceedings of the Royal Society B: Biological Sciences</em> 288 (1946): 20202513. <a href="https://doi.org/10.1098/rspb.2020.2513">https://doi.org/10.1098/rspb.2020.2513</a>.
</div>
<div id="ref-Hadjidemetriou2020impact" class="csl-entry" role="listitem">
Hadjidemetriou, Georgios M., Manu Sasidharan, Georgia Kouyialis, and Ajith K. Parlikad. 2020. <span>“The Impact of Government Measures and Human Mobility Trend on <span>COVID</span>-19 Related Deaths in the <span>UK</span>.”</span> <em>Transportation Research Interdisciplinary Perspectives</em> 6 (July): 100167. <a href="https://doi.org/10.1016/j.trip.2020.100167">https://doi.org/10.1016/j.trip.2020.100167</a>.
</div>
<div id="ref-Hornberg2021Impact" class="csl-entry" role="listitem">
Hornberg, Jonas, Timo Haselhoff, Bryce T. Lawrence, Jonas L. Fischer, Salman Ahmed, Dietwald Gruehn, and Susanne Moebus. 2021. <span>“Impact of the <span>COVID</span>-19 Lockdown Measures on Noise Levels in Urban Areas<span></span>a Pre/During Comparison of Long-Term Sound Pressure Measurements in the <span>R</span>uhr <span>A</span>rea, <span>G</span>ermany.”</span> <em>International Journal of Environmental Research and Public Health</em> 18 (9): 4653. <a href="https://doi.org/10.3390/ijerph18094653">https://doi.org/10.3390/ijerph18094653</a>.
</div>
<div id="ref-IEC2013Electroacoustics" class="csl-entry" role="listitem">
IEC 61672-1:2013. 2013. <span>“<span class="nocase">Electroacoustics – Sound level mdeters – Part 1: Specifications</span>.”</span> <a href="https://webstore.iec.ch/publication/5708">https://webstore.iec.ch/publication/5708</a>.
</div>
<div id="ref-ISO12913Part1" class="csl-entry" role="listitem">
ISO 12913-1:2014. 2014. <span>“<span>Acoustics</span> – <span>Soundscape</span> – <span>Part</span> 1: <span>Definition</span> and Conceptual Framework.”</span>
</div>
<div id="ref-ISO2016Acoustics" class="csl-entry" role="listitem">
ISO 1996-1:2016. 2016. <span>“<span class="nocase">Acoustics – Description, measurement and assessment of environmental noise – Part 1: Basic quantities and assessment procedures</span>.”</span> <a href="https://www.iso.org/standard/59765.html">https://www.iso.org/standard/59765.html</a>.
</div>
<div id="ref-ISO2017Acoustics" class="csl-entry" role="listitem">
ISO 532-1:2017. 2017. <span>“<span class="nocase">Acoustics – Methods for calculating loudness – Part 1: Zwicker method</span>.”</span> <a href="https://www.iso.org/standard/63077.html">https://www.iso.org/standard/63077.html</a>.
</div>
<div id="ref-ISO12913Part2" class="csl-entry" role="listitem">
ISO/TS 12913-2:2018. 2018. <span>“<span>Acoustics</span> – <span>Soundscape</span> – <span>Part</span> 2: <span>Data</span> Collection and Reporting Requirements.”</span>
</div>
<div id="ref-ISO12913Part3" class="csl-entry" role="listitem">
ISO/TS 12913-3:2019. 2019. <span>“<span>Acoustics</span> – <span>Soundscape</span> – <span>Part</span> 3: <span>Data</span> Analysis.”</span>
</div>
<div id="ref-Kang2006Urban" class="csl-entry" role="listitem">
Kang, Jian. 2006. <em>Urban <span>S</span>ound <span>E</span>nvironment</em>. <span>CRC</span> Press. <a href="https://doi.org/10.1201/9781482265613">https://doi.org/10.1201/9781482265613</a>.
</div>
<div id="ref-Kang2018Impact" class="csl-entry" role="listitem">
Kang, Jian, and Francesco Aletta. 2018. <span>“<span class="nocase">The Impact and Outreach of Soundscape Research</span>.”</span> <em>Environments</em> 5 (5): 58. <a href="https://doi.org/10.3390/environments5050058">https://doi.org/10.3390/environments5050058</a>.
</div>
<div id="ref-Kuznetsova2017lmerTest" class="csl-entry" role="listitem">
Kuznetsova, Alexandra, Per B. Brockhoff, and Rune H. B. Christensen. 2017. <span>“<span class="nocase">lmerTest</span> Package: Tests in Linear Mixed Effects Models.”</span> <em>Journal of Statistical Software</em> 82 (13). <a href="https://doi.org/10.18637/jss.v082.i13">https://doi.org/10.18637/jss.v082.i13</a>.
</div>
<div id="ref-Lenzi2021Soundscape" class="csl-entry" role="listitem">
Lenzi, Sara, Juan Sádaba, and PerMagnus Lindborg. 2021. <span>“Soundscape in Times of Change: <span>C</span>ase Study of a City Neighbourhood During the <span>COVID</span>-19 Lockdown.”</span> <em>Frontiers in Psychology</em> 12 (March). <a href="https://doi.org/10.3389/fpsyg.2021.570741">https://doi.org/10.3389/fpsyg.2021.570741</a>.
</div>
<div id="ref-Lionello2020systematic" class="csl-entry" role="listitem">
Lionello, Matteo, Francesco Aletta, and Jian Kang. 2020. <span>“<span class="nocase">A systematic review of prediction models for the experience of urban soundscapes</span>.”</span> <em>Applied Acoustics</em> 170 (June). <a href="https://doi.org/10.1016/j.apacoust.2020.107479">https://doi.org/10.1016/j.apacoust.2020.107479</a>.
</div>
<div id="ref-Lionello2021Introducing" class="csl-entry" role="listitem">
Lionello, Matteo, Francesco Aletta, Andrew Mitchell, and Jian Kang. 2021. <span>“<span class="nocase">Introducing a Method for Intervals Correction on Multiple Likert Scales: A Case Study on an Urban Soundscape Data Collection Instrument</span>.”</span> <em>Frontiers in Psychology</em> 11: 3943. <a href="https://doi.org/10.3389/fpsyg.2020.602831">https://doi.org/10.3389/fpsyg.2020.602831</a>.
</div>
<div id="ref-Luedecke2021sjPlot" class="csl-entry" role="listitem">
Lüdecke, Daniel. 2021. <em>sjPlot: Data Visualization for Statistics in Social Science</em>. <a href="https://CRAN.R-project.org/package=sjPlot">https://CRAN.R-project.org/package=sjPlot</a>.
</div>
<div id="ref-McKnight2010Mann" class="csl-entry" role="listitem">
McKnight, Patrick E., and Julius Najab. 2010. <span>“Mann-Whitney <span>U</span> Test.”</span> John Wiley <span>&amp;</span> Sons, Inc. <a href="https://doi.org/10.1002/9780470479216.corpsy0524">https://doi.org/10.1002/9780470479216.corpsy0524</a>.
</div>
<div id="ref-Mitchell2020Soundscape" class="csl-entry" role="listitem">
Mitchell, Andrew, Tin Oberman, Francesco Aletta, Mercede Erfanian, Magdalena Kachlicka, Matteo Lionello, and Jian Kang. 2020. <span>“<span class="nocase">The Soundscape Indices (SSID) Protocol: A Method for Urban Soundscape Surveys–Questionnaires with Acoustical and Contextual Information</span>.”</span> <em>Applied Sciences</em> 10 (7): 2397. <a href="https://doi.org/10.3390/app10072397">https://doi.org/10.3390/app10072397</a>.
</div>
<div id="ref-Mitchell2021International" class="csl-entry" role="listitem">
———. 2021. <span>“<span class="nocase">The International Soundscape Database: An integrated multimedia database of urban soundscape surveys – questionnaires with acoustical and contextual information</span>.”</span> Zenodo. <a href="https://doi.org/10.5281/zenodo.5578572">https://doi.org/10.5281/zenodo.5578572</a>.
</div>
<div id="ref-Munoz2020Lockdown" class="csl-entry" role="listitem">
Munoz, Patricio, Bruno Vincent, Céline Domergue, Vincent Gissinger, Sébastien Guillot, Yann Halbwachs, and Valérie Janillon. 2020. <span>“Lockdown During <span>COVID</span>-19 Pandemic: <span>I</span>mpact on Road Traffic Noise and on the Perception of Sound Environment in <span>F</span>rance.”</span> <em>Noise Mapping</em> 7 (1): 287–302. <a href="https://doi.org/10.1515/noise-2020-0024">https://doi.org/10.1515/noise-2020-0024</a>.
</div>
<div id="ref-Orga2021Multilevel" class="csl-entry" role="listitem">
Orga, Ferran, Andrew Mitchell, Marc Freixes, Francesco Aletta, Rosa Ma Alsina-Pagès, and Maria Foraster. 2021. <span>“<span class="nocase">Multilevel Annoyance Modelling of Short Environmental Sound Recordings</span>.”</span> <em>Sustainability</em> 13 (11): 5779. <a href="https://doi.org/10.3390/su13115779">https://doi.org/10.3390/su13115779</a>.
</div>
<div id="ref-Parker2020Anthropause" class="csl-entry" role="listitem">
Parker, Murray, and Dirk H. R. Spennemann. 2020. <span>“<span class="nocase">Anthropause on audio: The effects of the COVID-19 pandemic on church bell ringing and associated soundscapes in New South Wales (Australia)</span>.”</span> <em>The Journal of the Acoustical Society of America</em> 148 (5): 3102–6. <a href="https://doi.org/10.1121/10.0002451">https://doi.org/10.1121/10.0002451</a>.
</div>
<div id="ref-RCT2018R" class="csl-entry" role="listitem">
R Core Team. 2018. <em><span>R</span>: <span>A</span> <span>L</span>anguage and <span>E</span>nvironment for <span>S</span>tatistical <span>C</span>omputing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-Radicchi2021Sound" class="csl-entry" role="listitem">
Radicchi, Antonella, Pınar Cevikayak Yelmi, Andy Chung, Pamela Jordan, Sharon Stewart, Aggelos Tsaligopoulos, Lindsay McCunn, and Marcus Grant. 2021. <span>“Sound and the Healthy City.”</span> <em>Cities <span>&amp;</span> Health</em> 5 (1-2): 1–13. <a href="https://doi.org/10.1080/23748834.2020.1821980">https://doi.org/10.1080/23748834.2020.1821980</a>.
</div>
<div id="ref-Ren2020Pandemic" class="csl-entry" role="listitem">
Ren, Xuefei. 2020. <span>“Pandemic and Lockdown: <span>A</span> Territorial Approach to <span>COVID</span>-19 in <span>C</span>hina, <span>I</span>taly and the <span>U</span>nited <span>S</span>tates.”</span> <em>Eurasian Geography and Economics</em> 61 (4-5): 423–34. <a href="https://doi.org/10.1080/15387216.2020.1762103">https://doi.org/10.1080/15387216.2020.1762103</a>.
</div>
<div id="ref-Ricciardi2015Sound" class="csl-entry" role="listitem">
Ricciardi, Paola, Pauline Delaitre, Catherine Lavandier, Francesca Torchia, and Pierre Aumond. 2015. <span>“<span class="nocase">Sound quality indicators for urban places in <span>Paris</span> cross-validated by <span>Milan</span> data</span>.”</span> <em>The Journal of the Acoustical Society of America</em> 138: 2337–48. <a href="https://doi.org/10.1121/1.4929747">https://doi.org/10.1121/1.4929747</a>.
</div>
<div id="ref-Rumpler2021Noise" class="csl-entry" role="listitem">
Rumpler, Romain, Siddharth Venkataraman, and Peter Göransson. 2021. <span>“Noise Measurements as a Proxy to Evaluating the Response to Recommendations in Times of Crisis: <span>A</span>n Update Analysis of the Transition to the Second Wave of the <span>CoViD</span>-19 Pandemic in <span>C</span>entral <span>S</span>tockholm, <span>S</span>weden.”</span> <em>The Journal of the Acoustical Society of America</em> 149 (3): 1838–42. <a href="https://doi.org/10.1121/10.0003778">https://doi.org/10.1121/10.0003778</a>.
</div>
<div id="ref-Sakagami2020How" class="csl-entry" role="listitem">
Sakagami, Kimihiro. 2020. <span>“How Did the ’State of Emergency’ Declaration in <span>Japan</span> Due to the <span>COVID-19</span> Pandemic Affect the Acoustic Environment in a Rather Quiet Residential Area?”</span> <em>UCL Open Environment</em>, no. 1: 1–9. <a href="https://doi.org/10.14324/111.444/ucloe.000009">https://doi.org/10.14324/111.444/ucloe.000009</a>.
</div>
<div id="ref-Sottek2005Models" class="csl-entry" role="listitem">
Sottek, Roland, and Klaus Genuit. 2005. <span>“Models of Signal Processing in Human Hearing.”</span> <em><span>AEU</span> - International Journal of Electronics and Communications</em> 59 (3): 157–65. <a href="https://doi.org/10.1016/j.aeue.2005.03.016">https://doi.org/10.1016/j.aeue.2005.03.016</a>.
</div>
<div id="ref-Tarlao2020Investigating" class="csl-entry" role="listitem">
Tarlao, Cynthia, Jochen Steffens, and Catherine Guastavino. 2020. <span>“Investigating Contextual Influences on Urban Soundscape Evaluations with Structural Equation Modeling.”</span> <em>Building and Environment</em> 188 (November). <a href="https://doi.org/10.1016/j.buildenv.2020.107490">https://doi.org/10.1016/j.buildenv.2020.107490</a>.
</div>
<div id="ref-Torresin2020Indoor" class="csl-entry" role="listitem">
Torresin, Simone, Rossano Albatici, Francesco Aletta, Francesco Babich, Tin Oberman, Stefano Siboni, and Jian Kang. 2020. <span>“<span class="nocase">Indoor soundscape assessment: A principal components model of acoustic perception in residential buildings</span>.”</span> <em>Building and Environment</em> 182 (September): 107152. <a href="https://doi.org/10.1016/j.buildenv.2020.107152">https://doi.org/10.1016/j.buildenv.2020.107152</a>.
</div>
<div id="ref-Truax1999Handbook" class="csl-entry" role="listitem">
Truax, Barry. 1999. <em>Handbook for <span>A</span>coustic <span>E</span>cology</em>. Cambridge, MA: Cambridge Street Publishing.
</div>
<div id="ref-VidaManzano2021sound" class="csl-entry" role="listitem">
Vida Manzano, Jerónimo, José Antonio Almagro Pastor, Rafael Garcı́a Quesada, Francesco Aletta, Tin Oberman, Andrew Mitchell, and Jian Kang. 2021. <span>“The "Sound of Silence" in <span>G</span>ranada During the <span>COVID-19</span> Lockdown.”</span> <em>Noise Mapping</em> 8 (1): 16–31. <a href="https://doi.org/10.1515/noise-2021-0002">https://doi.org/10.1515/noise-2021-0002</a>.
</div>
<div id="ref-Waskom2021seaborn" class="csl-entry" role="listitem">
Waskom, Michael L. 2021. <span>“Seaborn: Statistical Data Visualization.”</span> <em>Journal of Open Source Software</em> 6 (60): 3021. <a href="https://doi.org/10.21105/joss.03021">https://doi.org/10.21105/joss.03021</a>.
</div>
<div id="ref-Woods2017Headphone" class="csl-entry" role="listitem">
Woods, Kevin J. P., Max H. Siegel, James Traer, and Josh H. McDermott. 2017. <span>“Headphone Screening to Facilitate Web-Based Auditory Experiments.”</span> <em>Attention, Perception, <span>&amp;</span> Psychophysics</em> 79 (7): 2064–72. <a href="https://doi.org/10.3758/s13414-017-1361-2">https://doi.org/10.3758/s13414-017-1361-2</a>.
</div>
<div id="ref-WMA2013World" class="csl-entry" role="listitem">
World Medical Association. 2013. <span>“World <span>M</span>edical <span>A</span>ssociation <span>D</span>eclaration of <span>H</span>elsinki: <span>E</span>thical Principles for Medical Research Involvinghuman Subjects.”</span> <em><span>JAMA</span></em> 310 (20): 2191. <a href="https://doi.org/10.1001/jama.2013.281053">https://doi.org/10.1001/jama.2013.281053</a>.
</div>
<div id="ref-Yang2005Acoustic" class="csl-entry" role="listitem">
Yang, W., and J. Kang. 2005. <span>“Acoustic Comfort Evaluation in Urban Open Public Spaces.”</span> <em>Applied Acoustics</em> 66 (2): 211–29. <a href="https://doi.org/10.1016/j.apacoust.2004.07.011">https://doi.org/10.1016/j.apacoust.2004.07.011</a>.
</div>
<div id="ref-Zwicker2007Psychoacoustics" class="csl-entry" role="listitem">
Zwicker, Eberhard, and Hugo Fastl. 2007. <em><span class="nocase">Psychoacoustics: facts and models</span></em>. Third ed. Berlin ; New York: Springer. <a href="https://doi.org/10.1007/978-3-540-68888-4">https://doi.org/10.1007/978-3-540-68888-4</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>See <a href="https://zenodo.org/record/5654747" class="uri">https://zenodo.org/record/5654747</a> for “The International Soundscape Database (V0.2.1)” (Last viewed 11/9/21)<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See supplementary material at <a href="https://www.scitation.org/doi/suppl/10.1121/10.0008928" class="uri">https://www.scitation.org/doi/suppl/10.1121/10.0008928</a> for site descriptions per ISO/TS (2018) featuring the address, overall psychoacoustic characteristics of the location, typical use of each location, and pictures taken during the survey sessions.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>See <a href="https://zenodo.org/record/5654747" class="uri">https://zenodo.org/record/5654747</a> for “The International Soundscape Database (V0.2.1)” (Last viewed 11/9/21)<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>See supplementary material at <a href="https://www.scitation.org/doi/suppl/10.1121/10.0008928" class="uri">https://www.scitation.org/doi/suppl/10.1121/10.0008928</a> for site descriptions per ISO/TS (2018) featuring the address, overall psychoacoustic characteristics of the location, typical use of each location, and pictures taken during the survey sessions.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>See <a href="https://zenodo.org/record/5654747" class="uri">https://zenodo.org/record/5654747</a> for “The International Soundscape Database (V0.2.1)” (Last viewed 11/9/21)<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>See (&lt;www.gorilla.sc&gt;) (Last viewed 11/29/21)<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>See <a href="https://cordis.europa.eu/project/rcn/211802/factsheet/en" class="uri">https://cordis.europa.eu/project/rcn/211802/factsheet/en</a> (Last viewed 12/7/21)<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{mitchell2021,
  author = {Mitchell, Andrew and Oberman, Tin and Aletta, Francesco and
    Kachlicka, Magdalena and Lionello, Matteo and Erfanian, Mercede and
    Kang, Jian},
  title = {Investigating Urban Soundscapes of the {COVID-19} Lockdown:
    {A} Predictive Soundscape Modeling Approach},
  journal = {J. Acoust. Soc. Am.},
  volume = {150},
  number = {6},
  pages = {4474-\/-4488},
  date = {2021-12-28},
  url = {https://pubs.aip.org/asa/jasa/article/150/6/4474/995506/Investigating-urban-soundscapes-of-the-COVID-19},
  doi = {10.1121/10.0009794},
  langid = {en},
  abstract = {The unprecedented lockdowns due to COVID-19 in spring 2020
    triggered changes in human activities in public spaces. A predictive
    modeling approach was developed to characterize the changes in the
    perception of the sound environment when people could not be
    surveyed. Building on a database of soundscape questionnaires
    (\$N=1,136\$) and binaural recordings (\$N=687\$) collected in 13
    locations across London and Venice during 2019, new recordings
    (\$N=571\$) were made in the same locations during the 2020
    lockdowns. Using these 30-second-long recordings, linear multi-level
    models were developed to predict soundscape pleasantness
    (\$R\^{}2=0.85\$) and eventfulness (\$R\^{}2=0.715\$) during the
    lockdown and compare changes for each location. Performance was
    above average for comparable models. An online listening study also
    investigated the change in sound sources within the spaces. Results
    indicate: 1) human sounds were less dominant and natural sounds more
    dominant across all locations; 2) contextual information is
    important for predicting pleasantness but not for eventfulness; 3)
    perception shifted towards less eventful soundscapes and to more
    pleasant soundscapes for previously traffic-dominated locations, but
    not for human- and natural-dominated locations. This study
    demonstrates the usefulness of predictive modeling and the
    importance of considering contextual information when discussing the
    impact of sound level reductions on the soundscape.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-mitchell2021" class="csl-entry quarto-appendix-citeas" role="listitem">
Mitchell, Andrew, Tin Oberman, Francesco Aletta, Magdalena Kachlicka,
Matteo Lionello, Mercede Erfanian, and Jian Kang. 2021.
<span>“Investigating Urban Soundscapes of the COVID-19 Lockdown: A
Predictive Soundscape Modeling Approach .”</span> <em>J. Acoust. Soc.
Am.</em> 150 (December): 4474--4488. <a href="https://doi.org/10.1121/10.0009794">https://doi.org/10.1121/10.0009794</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/drandrewmitchell\.com\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<script src="../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>