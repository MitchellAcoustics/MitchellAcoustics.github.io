{
  "hash": "0b49ce927a09df9b330c07cd8cc0e53b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: |\n  Sampling, Experiments, \\\n  and Probability\ndate: \"2025-01-30\"\ndescription: Introductory lecture on Probability and Sampling for BSSC0021 - Business Statistics and Data Analytics.\nsidebar: false\nimage: https://statsthinking21.github.io/statsthinking21-core-site/images/conditional_probability.png\ncategories: \n  - lecture\n\nformat:\n  clean-revealjs: \n    self-contained: true\n    self-contained-math: true\n    slide-number: true\n    show-slide-number: all\n    progress: true\n    transition: slide\n    background-transition: fade\n    preview-links: auto\n    width: 1247\n    height: 810\n    scrollable: true\n\nengine: knitr\neditor:\n  render-on-save: true\nexecute:\n  cache: true\n\ntitle-slide-attributes: \n  data-background-image: https://statsthinking21.github.io/statsthinking21-core-site/images/conditional_probability.png\n  data-background-opacity: \"0.7\"\n  data-background-position: 75% 50%\n  data-background-size: 40%\n  \ncitation:\n  type: paper-conference\n  genre: Lecture\n  title: Sampling, Experiments, and Probability\n  author:\n    - name: Andrew Mitchell\n  container-title: Lecture for BSSC0021 - Business Statistics and Data Analytics\n---\n\n\n\n\n\n## Learning Objectives {.r-fit-text}\n\nAfter this lecture, you should be able to:\n\n::: incremental\n{{< fa dice >}} Describe the sample space for a random experiment\n\n{{< fa chart-simple >}} Compute relative frequency and empirical probability\n\n{{< fa diagram-successor >}} Understand probability rules for events and\ncombinations\n\n{{< fa arrow-trend-up >}} Explain the law of large numbers\n\n{{< fa network-wired >}} Understand conditional probability and independence\n\n{{< fa equals >}} Use Bayes' theorem for real-world applications\n:::\n\n::: aside\nThis lecture is adapted from Chapters 6 and 7 of @Poldrack2023Statistical.\n:::\n\n::: notes\nThese learning objectives are carefully chosen to focus on practical\nunderstanding rather than mathematical rigor. They align with the chapter's\ngoals while remaining accessible to non-technical students.\n\nKey teaching points:\n\n-   Emphasize practical understanding over mathematical formalism\n-   Focus on concepts that will be useful in business and data analysis\n-   Build foundation for later statistical concepts\n:::\n\n# Probability {background-color=\"#40666e\"}\n\n## Introduction {.r-fit-text}\n\nProbability theory is the branch of mathematics that deals with chance and\nuncertainty. It forms an important part of the foundation for statistics,\nbecause it provides us with the mathematical tools to describe uncertain events.\n\n::: incremental\n-   **Historical Origins**\n    -   Developed by mathematicians studying gambling\n    -   Key advances from Pascal and Fermat's correspondence\n    -   Evolved into fundamental tool for statistics\n-   **Modern Applications**\n    -   Business decision-making under uncertainty\n    -   Weather forecasting and risk assessment\n    -   Medical diagnosis and testing\n    -   Data analysis and machine learning\n:::\n\n::: notes\nThe study of probability arose in part due to interest in understanding games of\nchance, like cards or dice. These games provide useful examples of many\nstatistical concepts, because when we repeat these games the likelihood of\ndifferent outcomes remains (mostly) the same.\n\nHistorical context:\n\n-   Early probability theory developed by mathematicians studying gambling\n-   Pascal and Fermat's correspondence about gambling problems\n-   Evolution into a fundamental tool for statistics and data analysis\n\nKey points to emphasize:\n\n-   Probability helps us quantify uncertainty\n-   Games of chance provide consistent, repeatable examples\n-   Forms the foundation for all statistical analysis\n-   Relevant to business decisions under uncertainty\n\nTeaching tip: Start by asking students about their intuitive understanding of\nprobability - they likely encounter it daily in weather forecasts, sports\nstatistics, or business reports.\n\nCommon misconceptions to address:\n\n-   Probability is not just about gambling\n-   While based in mathematics, focus will be on practical understanding\n-   Importance in modern data analysis and decision making\n:::\n\n## What is Probability?\n\n-   A number describing the likelihood of an event occurring\n-   Ranges from 0 (impossibility) to 1 (certainty)\n-   Sometimes expressed as percentages (0% to 100%)\n\n::: fragment\n**Examples from everyday life:**\n\n-   Weather forecast: \"20% chance of rain today\"\n-   Sports: Steph Curry's 91% free throw success rate\n-   Medical tests: PSA test with 80% sensitivity\n:::\n\n::: notes\nInformally, we usually think of probability as a number that describes the\nlikelihood of some event occurring, which ranges from zero (impossibility) to\none (certainty). Sometimes probabilities will instead be expressed in\npercentages, which range from zero to one hundred.\n\nKey concepts to emphasize:\n\n1.  Probability scale:\n\n    -   0 = impossible (e.g., rolling a 7 on a six-sided die)\n    -   1 = certain (e.g., rolling a number between 1 and 6 on a six-sided die)\n    -   Most real-world events fall between these extremes\n\n2.  Everyday encounters with probability:\n\n    -   Weather forecasts: Percentage chance of rain\n    -   Sports statistics: Player performance rates\n    -   Medical tests: Accuracy rates\n    -   Insurance: Risk assessments\n\n3.  Common misconceptions:\n\n    -   A 20% chance of rain doesn't mean it will rain 20% of the day\n    -   Probabilities don't predict individual outcomes, only long-term patterns\n    -   Even very unlikely events can happen (and do happen regularly)\n\n4.  Historical context:\n\n    -   Early probability theory developed through analysis of games of chance\n    -   Modern applications far beyond gambling\n    -   Critical role in scientific research and data analysis\n\nTeaching tips:\n\n-   Ask students about their daily encounters with probability\n-   Discuss how they interpret weather forecasts\n-   Challenge intuitive misconceptions about probability\n-   Connect to real-world decision-making under uncertainty\n\nTime allocation: Spend about 3-4 minutes on this slide to establish fundamental\nconcepts that will be built upon throughout the lecture.\n:::\n\n## Key Terms\n\n:::::: panel-tabset\n### 1. Experiment\n\n**Any activity that produces or observes an outcome**\n\n::: incremental\n-   Simple Examples:\n    -   Flipping a coin\n    -   Rolling a die\n    -   Drawing a card\n-   Real-world Examples:\n    -   Trying a new route to work\n    -   Testing a medical treatment\n    -   Measuring customer satisfaction\n:::\n\n### 2. Sample Space\n\n**Set of all possible outcomes**\n\n::: incremental\n-   Discrete Sets:\n    -   Coin flip: {heads, tails}\n    -   Six-sided die: {1,2,3,4,5,6}\n    -   Card draw: {52 possible cards}\n-   Continuous Sets:\n    -   Time to work: all real numbers \\> 0\n    -   Temperature readings\n    -   Stock prices\n:::\n\n### 3. Event\n\n**A subset of the sample space**\n\n::: incremental\n-   Single Events:\n    -   Getting heads in a coin flip\n    -   Rolling a 4 on a die\n    -   Drawing the ace of spades\n-   Compound Events:\n    -   Rolling an even number\n    -   Drawing a red card\n    -   Arriving at work in under 20 minutes\n:::\n::::::\n\n::: notes\nTo formalize probability theory, we need these precise definitions:\n\nAn experiment is any activity that produces or observes an outcome. Examples are\nflipping a coin, rolling a 6-sided die, or trying a new route to work to see if\nit's faster than the old route.\n\nThe sample space is the set of possible outcomes for an experiment. We represent\nthese by listing them within a set of squiggly brackets. For a coin flip, the\nsample space is {heads, tails}. For a six-sided die, the sample space is each of\nthe possible numbers that can appear: {1,2,3,4,5,6}. For the amount of time it\ntakes to get to work, the sample space is all possible real numbers greater than\nzero (since it can't take a negative amount of time to get somewhere, at least\nnot yet).\n\nAn event is a subset of the sample space. In principle it could be one or more\nof possible outcomes in the sample space, but here we will focus primarily on\nelementary events which consist of exactly one possible outcome.\n:::\n\n## Formal Properties of Probability {.smaller}\n\nKolmogorov's axioms define what makes a value a probability:\n\n::::::: columns\n:::: {.column width=\"50%\"}\n::: incremental\n1.  **Non-negativity**: Probability cannot be negative\n    -   $P(E_i) \\ge 0$\n    -   Example: Can't have -10% chance of rain\n    -   All probabilities must be zero or positive\n2.  **Total Probability**: All outcomes sum to 1\n    -   $\\sum_{i=1}^N{P(E_i)} = 1$\n    -   Example: Rolling a die\n        -   P(1) + P(2) + P(3) + P(4) + P(5) + P(6) = 1\n        -   Each P(number) = 1/6, so 6 \\* (1/6) = 1\n3.  **Upper Bound**: Individual probability ≤ 1\n    -   $P(E_i)\\le 1$\n    -   Follows from rules 1 and 2\n    -   Example: Can't have 120% chance of success\n:::\n::::\n\n:::: {.column width=\"50%\"}\n::: fragment\n**Why These Rules Matter:**\n\n-   Ensure mathematical consistency\n-   Enable probability calculations\n-   Form foundation for statistics\n:::\n::::\n:::::::\n\n::: notes\nThese formal features of probability were first defined by the Russian\nmathematician Andrei Kolmogorov. These are the features that a value has to have\nif it is going to be a probability.\n\nLet's say that we have a sample space defined by N independent events,\n${E_1, E_2, ... , E_N}$, and $X$ is a random variable denoting which of the\nevents has occurred. $P(X=E_i)$ is the probability of event $i$.\n\nThe second point means that if we take the probability of each Ei and add them\nup, they must sum to 1. The third point is implied by the second; since they\nmust sum to one, and they can't be negative, then any particular probability\ncannot exceed one.\n:::\n\n## How Do We Determine Probabilities? {.smaller}\n\nThree main approaches:\n\n::: incremental\n### Personal Belief\n\n-   Based on knowledge and experience\n-   Example: Bernie Sanders winning 2016 election\n-   Subjective but sometimes necessary\n-   Cannot verify through experiments\n\n### Empirical Frequency\n\n-   Based on actual data collection\n-   Example: Rain in San Francisco\n    -   73 rainy days in 2017\n    -   P(rain) = 73/365 = 0.2\n-   Requires sufficient data\n\n### Classical Probability\n\n-   Based on equally likely outcomes\n-   Example: Fair six-sided die\n    -   P(rolling a 6) = 1/6\n-   Mathematical approach\n:::\n\n## Probability Distributions: Free Throw Example {.smaller}\n\nSteph Curry's free throw success in 4 attempts:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Simple and cumulative probability distributions\n\n| numSuccesses| Probability| CumulativeProbability|\n|------------:|-----------:|---------------------:|\n|            0|      0.0001|                0.0001|\n|            1|      0.0027|                0.0027|\n|            2|      0.0402|                0.0430|\n|            3|      0.2713|                0.3143|\n|            4|      0.6857|                1.0000|\n\n\n:::\n:::\n\n\n\n::: incremental\n-   **Understanding the Table**:\n    -   numSuccesses: Number of successful free throws\n    -   Probability: Chance of exactly that many successes\n    -   CumulativeProbability: Chance of that many or fewer successes\n-   **Key Insights**:\n    -   Most likely outcome: 4 successes (0.6853)\n    -   Very unlikely to make 0-2 shots (0.0421)\n    -   Shows why Curry is considered elite\n:::\n\n::: notes\n1.  Personal Belief:\n\n    -   Used when we can't do the actual experiment\n    -   Example: What if Bernie Sanders had been the Democratic nominee in 2016?\n    -   Based on knowledge of politics, polls, historical data\n    -   Limitations: Subjective, can vary between experts\n    -   Often the only available method for unique events\n\n2.  Empirical Frequency:\n\n    -   Most scientific approach when data is available\n    -   San Francisco rain example:\n        -   Define the experiment: Check daily rain data\n        -   Count outcomes: 73 rainy days in 2017\n        -   Calculate probability: 73/365 = 0.2\n    -   Advantages:\n        -   Based on actual data\n        -   Can be verified and repeated\n    -   Limitations:\n        -   Requires sufficient data\n        -   Past data may not predict future events\n        -   Sample size affects accuracy\n\n3.  Classical Probability:\n\n    -   Based on equally likely outcomes\n    -   Examples from games of chance:\n        -   Die rolls: Each number has 1/6 probability\n        -   Coin flips: Heads and tails each 1/2\n    -   Advantages:\n        -   Can calculate without data collection\n        -   Precise mathematical basis\n    -   Limitations:\n        -   Requires truly equal probabilities\n        -   Rare in real-world situations\n\nTeaching tips:\n\n-   Start with personal belief examples to engage students\n-   Use San Francisco rain data to show how empirical probability works\n-   Use dice/coins to demonstrate classical probability\n-   Emphasize that different methods suit different situations\n\nTime allocation: 8 minutes for this section\n\n-   2-3 minutes for personal belief\n-   3-4 minutes for empirical frequency with SF example\n-   2-3 minutes for classical probability\n:::\n\n## Law of Large Numbers {.smaller}\n\nAs we increase the number of trials, our empirical probability approaches the\ntrue probability.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n::: incremental\n-   **Understanding the Graph:**\n    -   Blue line: True probability (0.5)\n    -   Black line: Observed probability\n    -   High variability with small samples\n    -   Converges to true value over time\n-   **The \"Law of Small Numbers\":**\n    -   Common mistake: treating small samples like large ones\n    -   Term coined by Kahneman and Tversky\n    -   Even trained researchers make this error\n    -   Early results can be misleading\n:::\n\n::: fragment\n**Real-World Example: 2017 Alabama Senate Election**\n\n-   Early results highly volatile\n    -   Initial large lead for Jones\n    -   Switched to Moore leading\n    -   Finally Jones won\n-   Perfect illustration of why we need sufficient data\n-   Applies to:\n    -   Opinion polls\n    -   Medical studies\n    -   Market research\n:::\n\n::: notes\nThe Law of Large Numbers is a fundamental principle in probability theory that\nhelps us understand why empirical probability works. It shows that as we\nincrease our sample size, our observed probability gets closer to the true\nprobability.\n\nKey Points: 1. Coin Flip Simulation:\n\n-   Blue dashed line shows true probability (0.5)\n-   Black line shows observed probability\n-   Notice high variability with small samples\n-   Convergence to true probability with more flips\n\n2.  Alabama Senate Election Example:\n\n    -   December 2017 special election between Doug Jones and Roy Moore\n    -   Early in the evening, vote counts were especially volatile\n    -   Initial large lead for Jones\n    -   Switched to long period where Moore had the lead\n    -   Finally Jones took the lead to win the race\n    -   Perfect illustration of why early results can be misleading\n\n3.  The \"Law of Small Numbers\":\n\n    -   Term coined by Kahneman and Tversky\n    -   Common mistake: treating small samples like large ones\n    -   Even trained researchers make this error\n    -   Important implications for research and decision-making\n\n4.  Practical Implications:\n\n    -   Need large samples for reliable probability estimates\n    -   Early results or small samples can be very misleading\n    -   Particularly important in:\n        -   Opinion polls\n        -   Medical studies\n        -   Market research\n        -   Quality control\n\nTeaching Tips:\n\n-   Use interactive examples to demonstrate variability\n-   Connect to students' experience with sports statistics\n-   Discuss implications for research and decision-making\n-   Emphasize why we need sufficient data for reliable conclusions\n\nTime allocation: 5 minutes\n\n-   2 minutes for coin flip simulation\n-   2 minutes for Alabama election example\n-   1 minute for implications and discussion\n:::\n\n## Classical Probability: de Méré's Problem {.smaller}\n\nA famous gambling problem that helped develop probability theory:\n\n::: incremental\n1.  **First Game**: At least one six in four dice rolls\n\n    -   de Méré's calculation: 4 \\* (1/6) = 2/3\n    -   Actual probability: 1 - (5/6)⁴ = 0.517\n    -   He made money on this bet!\n\n2.  **Second Game**: At least one double-six in 24 rolls of two dice\n\n    -   de Méré's calculation: 24 \\* (1/36) = 2/3\n    -   Actual probability: 1 - (35/36)²⁴ = 0.491\n    -   He lost money on this bet\n:::\n\n## de Méré's Problem: Visual Analysis {.smaller}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n::: incremental\n-   Matrix shows all possible outcomes of two dice throws\n-   Red cells: Getting a six on either throw\n-   Blue cells: No sixes\n-   White cell (6,6): Double six counted only once\n-   Shows why simple addition is wrong\n:::\n\n::: notes\nThis historical example introduces several key probability concepts:\n\n1.  The Problem:\n\n    -   Chevalier de Méré was a French gambler\n    -   Played two different dice games\n    -   Consulted mathematician Blaise Pascal\n    -   Led to development of probability theory\n\n2.  de Méré's Error:\n\n    -   Simply added individual probabilities\n    -   Didn't account for overlap in multiple events\n    -   Shows why we need formal probability rules\n    -   Common mistake even today\n\n3.  Pascal's Solution:\n\n    -   Instead of calculating success directly\n    -   Calculated probability of no success\n    -   Then used complement rule: P(success) = 1 - P(no success)\n    -   For first game: 1 - (5/6)⁴ = 0.517\n    -   For second game: 1 - (35/36)²⁴ = 0.491\n\n4.  Key Lessons:\n\n    -   Simple addition of probabilities often wrong\n    -   Multiple events require careful calculation\n    -   Sometimes easier to calculate complement\n    -   Small differences in probability matter\n    -   Mathematical analysis can reveal gambling errors\n\nTeaching Tips:\n\n-   Use this to introduce probability rules\n-   Show why intuition can be misleading\n-   Demonstrate practical value of mathematical analysis\n-   Connect to modern probability applications\n\nTime allocation: 7 minutes\n\n-   2 minutes for historical context\n-   3 minutes for calculations\n-   2 minutes for implications\n:::\n\n## Conditional Probability {.smaller}\n\nThe probability of an event occurring, given that another event has occurred.\n\n::: incremental\n-   **Formula**: $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n    -   $P(A|B)$ reads as \"probability of A given B\"\n    -   $P(A \\cap B)$ is the joint probability\n    -   $P(B)$ is the overall probability of B\n-   **Visual Representation**:\n    -   Total population splits into groups (e.g., voters by party)\n    -   Each group further splits by outcome (e.g., voting choice)\n    -   Conditional probability focuses on one branch\n:::\n\n## Health Data Example {.smaller}\n\nNHANES data on physical activity and diabetes:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Joint probabilities for Diabetes and Physical Activity\n\n|Diabetes |PhysActive |    n|      prob|\n|:--------|:----------|----:|---------:|\n|No       |No         | 2123| 0.3900423|\n|No       |Yes        | 2770| 0.5089105|\n|Yes      |No         |  349| 0.0641191|\n|Yes      |Yes        |  201| 0.0369282|\n\n\n:::\n:::\n\n\n\n::: incremental\n-   **Understanding the Data**:\n    -   Joint probabilities show overlap between conditions\n    -   Can calculate: $P(diabetes|inactive)$\n    -   Shows real-world health relationships\n-   **Key Insights**:\n    -   Physical activity associated with lower diabetes risk\n    -   Example of how conditional probability informs health research\n    -   Useful for public health recommendations\n:::\n\n::: notes\nSo far we have limited ourselves to simple probabilities - that is, the\nprobability of a single event or combination of events. However, we often wish\nto determine the probability of some event given that some other event has\noccurred, which are known as conditional probabilities.\n\nLet's take the 2016 US Presidential election as an example. There are two simple\nprobabilities that we could use to describe the electorate. First, we know the\nprobability that a voter in the US is affiliated with the Republican party:\np(Republican) = 0.44. We also know the probability that a voter cast their vote\nin favor of Donald Trump: p(Trump voter)=0.46. However, let's say that we want\nto know the following: What is the probability that a person cast their vote for\nDonald Trump, given that they are a Republican?\n\nTo compute the conditional probability of A given B (which we write as P(A\\|B),\n\"probability of A, given B\"), we need to know the joint probability (that is,\nthe probability of both A and B occurring) as well as the overall probability of\nB.\n:::\n\n## Independence {.smaller}\n\nTwo events are independent if:\n\n$P(A|B) = P(A)$\n\n::: fragment\n**Example 1: Political Independence**\n\n-   California vs proposed state of Jefferson\n-   $P(Jeffersonian) = 0.014$\n-   $P(Californian) = 0.986$\n-   Not independent: If you're Jeffersonian, you can't be Californian!\n:::\n\n## Independence: Health Example {.smaller}\n\nNHANES data on physical activity and mental health:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Mental health status by physical activity level\n\n|PhysActive | Bad Mental Health| Good Mental Health| Total|\n|:----------|-----------------:|------------------:|-----:|\n|No         |               629|               2510|  3139|\n|Yes        |               471|               3095|  3566|\n|Total      |              1100|               5605|  6705|\n\n\n:::\n:::\n\n\n\n::: incremental\n-   **Testing Independence**:\n    -   Compare $P(bad\\ mental\\ health|active)$ vs $P(bad\\ mental\\ health)$\n    -   If equal, variables would be independent\n    -   Data shows they are not independent\n    -   Physical and mental health are related\n:::\n\n::: notes\nThe term \"independent\" has a very specific meaning in statistics, which is\nsomewhat different from the common usage of the term. Statistical independence\nbetween two variables means that knowing the value of one variable doesn't tell\nus anything about the value of the other.\n\nLooking at it this way, we see that many cases of what we would call\n\"independence\" in the real world are not actually statistically independent. For\nexample, there is currently a move by a small group of California citizens to\ndeclare a new independent state called Jefferson, which would comprise a number\nof counties in northern California and Oregon. If this were to happen, then the\nprobability that a current California resident would now live in the state of\nJefferson would be P(Jeffersonian)=0.014, whereas the probability that they\nwould remain a California resident would be P(Californian)=0.986.\n\nThe new states might be politically independent, but they would not be\nstatistically independent, because if we know that a person is Jeffersonian,\nthen we can be sure that they are not Californian! Statistical independence\nrefers to the case where one cannot predict anything about one variable from the\nvalue of another variable. For example, knowing a person's hair color is\nunlikely to tell you whether they prefer chocolate or strawberry ice cream.\n:::\n\n## Bayes' Rule {.smaller}\n\nA powerful tool for updating probabilities based on new evidence:\n\n::: incremental\n-   **Basic Form**: $P(B|A) = \\frac{P(A|B)*P(B)}{P(A)}$\n\n-   **Expanded Form**:\n    $P(B|A) = \\frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)}$\n\n-   **Components**:\n\n    -   $P(B|A)$ = Updated probability (posterior)\n    -   $P(A|B)$ = Likelihood of evidence\n    -   $P(B)$ = Initial probability (prior)\n    -   $P(A)$ = Overall probability of evidence\n:::\n\n::: fragment\n**Medical Screening Example: PSA Test**\n\n-   Given:\n    -   Sensitivity = $P(positive|cancer) = 0.8$\n    -   Specificity = $P(negative|no\\ cancer) = 0.7$\n    -   Base rate = $P(cancer) = 0.058$\n-   Calculation:\n    -   $P(cancer|positive) = \\frac{0.8 * 0.058}{0.8*0.058 + 0.3*0.942}$\n    -   $= 0.14$ (only 14% chance of cancer)\n    -   Shows importance of considering base rates!\n:::\n\n::: notes\nIn many cases, we know P(A\\|B) but we really want to know P(B\\|A). This commonly\noccurs in medical screening, where we know P(positive test result\\|disease) but\nwhat we want to know is P(disease\\|positive test result).\n\nFor example, some doctors recommend that men over the age of 50 undergo\nscreening using a test called prostate specific antigen (PSA) to screen for\npossible prostate cancer. Before a test is approved for use in medical practice,\nthe manufacturer needs to test two aspects of the test's performance:\n\n1.  Sensitivity - how likely is it to find the disease when it is present\n2.  Specificity - how likely is it to give a negative result when there is no\n    disease present\n\nFor the PSA test, sensitivity is about 80% and specificity is about 70%.\nHowever, these don't answer the question that the physician wants to answer for\nany particular patient: what is the likelihood that they actually have cancer,\ngiven that the test comes back positive?\n\nUsing Bayes' rule with these numbers: P(cancer\\|test) = (0.8*0.058)/(0.8*0.058 +\n0.3\\*0.942) = 0.14\n\nThat's pretty small -- do you find that surprising? Many people do, and in fact\nthere is a substantial psychological literature showing that people\nsystematically neglect base rates (i.e. overall prevalence) in their judgments.\n:::\n\n## Odds and Odds Ratios {.smaller}\n\nConverting between probability and odds:\n\n::: incremental\n-   **Formulas**:\n    -   Odds = $\\frac{P(event)}{P(not\\ event)}$\n    -   Probability = $\\frac{odds}{1 + odds}$\n    -   Odds Ratio = $\\frac{posterior\\ odds}{prior\\ odds}$\n-   **PSA Test Example Calculations**:\n    -   Prior odds = $\\frac{0.058}{1-0.058} = 0.061$\n    -   Posterior odds = $\\frac{0.14}{1-0.14} = 0.16$\n    -   Odds ratio = $\\frac{0.16}{0.061} = 2.62$\n:::\n\n::: fragment\n**Interpreting Results**:\n\n-   Prior odds: 1:16 chance of cancer before test\n-   Posterior odds: 1:6 chance after positive test\n-   Odds ratio of 2.62 means:\n    -   Risk increased 2.62 times\n    -   But absolute risk still low (14%)\n    -   Shows why screening rare conditions is problematic\n:::\n\n::: notes\nThe result in the PSA example showed that the likelihood that the individual has\ncancer based on a positive PSA test result is still fairly low (0.14), even\nthough it's more than twice as big as it was before we knew the test result. We\nwould often like to quantify the relation between probabilities more directly,\nwhich we can do by converting them into odds which express the relative\nlikelihood of something happening or not.\n\nIn our PSA example:\n\nPrior odds = P(cancer)/P(not cancer) = 0.058/(1-0.058) = 0.061\n\nPosterior odds = 0.14/(1-0.14) = 0.16\n\nOdds ratio = 0.16/0.061 = 2.62\n\nThis tells us that the odds of having cancer are increased by 2.62 times given\nthe positive test result. An odds ratio is an example of what we will later call\nan effect size, which is a way of quantifying how relatively large any\nparticular statistical effect is.\n\nAs an aside, this is a reason why many medical researchers have become\nincreasingly wary of the use of widespread screening tests for relatively\nuncommon conditions; most positive results will turn out to be false positives,\nresulting in unnecessary followup tests with possible complications, not to\nmention added stress for the patient.\n:::\n\n## Summary\n\n::: incremental\n-   Probability quantifies uncertainty\n-   Three ways to determine probabilities\n-   Conditional probability for related events\n-   Bayes' rule for updating beliefs\n-   Importance of base rates\n:::\n\n::: notes\nKey takeaways from this lecture:\n\n1.  Probability theory provides mathematical tools to describe uncertain events\n2.  We can determine probabilities through:\n    -   Personal belief (subjective but sometimes necessary)\n    -   Empirical frequency (based on observed data)\n    -   Classical probability (based on equally likely outcomes)\n3.  The law of large numbers shows how empirical probability converges to true\n    probability\n4.  Conditional probability helps us understand related events\n5.  Bayes' rule allows us to update probabilities based on new evidence\n6.  Base rates are crucial but often neglected in probability judgments\n:::\n\n## Questions?\n\nThank you for your attention!\n\n::: notes\nSuggested readings for students interested in learning more:\n\n-   The Drunkard's Walk: How Randomness Rules Our Lives, by Leonard Mlodinow\n-   Ten Great Ideas about Chance, by Persi Diaconis and Brian Skyrms\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}